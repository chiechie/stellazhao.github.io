---
title:  è‡ªç„¶è¯­è¨€å¤„ç†7 Bert
author: chiechie
mathjax: true
date: 2021-04-26 11:50:30
tags: 
- NLP
- ç¥ç»ç½‘ç»œ
- æ¨¡å‹å¯è§†åŒ–
- Transformer
- Bert
- attention
categories:
- NLP
---

ä¸ºä»€ä¹ˆæœ‰äº†word2vecè¿˜ä¸å¤Ÿï¼Ÿè¿˜éœ€è¦bertå¹²å˜›ï¼Ÿå› ä¸ºä¸€è¯å¤šä¹‰ï¼ŒåŒæ ·çš„è¯åœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­è¡¨è¾¾çš„æ„æ€ä¸ä¸€æ ·ï¼Œè€Œword2vecçš„è¡¨ç¤ºæ˜¯é™æ€çš„ã€‚
æ‰€ä»¥æˆ‘ä»¬éœ€è¦ä¸€ä¸ªèƒ½å¯¹ä¸Šä¸‹æ–‡ç¼–ç çš„åŠ¨æ€çš„embeddingæ–¹æ³•ï¼Œbertå°±åº”è¿è€Œç”Ÿäº†ã€‚


# æ€»ç»“

- Bertä¹Ÿæ˜¯æ˜¯ä¸€ä¸ªå¯¹è¯è¯­ç”Ÿæˆè¯è¯­å‘é‡çš„æ–¹æ³•ã€‚
- Bertå¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒTransformerçš„encoderéƒ¨åˆ†
- Bertçš„å…¨ç§°æ˜¯Bidirectional Encoder Representations from Transformersï¼ŒåŒå‘encoderè¡¨ç¤º
- Bertçš„ç›®çš„æ˜¯é¢„è®­ç»ƒtransformeræ¨¡å‹çš„encoderç½‘ç»œï¼Œ
- Bertçš„å­¦ä¹ ç›®æ ‡æ˜¯æ€ä¹ˆè®¾å®šçš„å‘¢ï¼Ÿbertç”¨ä»¥ä¸‹ä¸¤ä¸ªä»»åŠ¡æ¥é¢„è®­ç»ƒtransformerä¸­çš„encoderç½‘ç»œ
    - ä»»åŠ¡1-é¢„æµ‹é®æŒ¡è¯ï¼ˆPredict Masked Wordsï¼‰ï¼š éšæœºé®æŒ¡ä¸Šä¸‹æ–‡ï¼Œè®©encoderæ ¹æ®ä¸Šä¸‹æ–‡æ¥é¢„æµ‹è¢«é®æŒ¡çš„å•è¯ï¼Œå¤§æ¦‚éšæœºé®æŒ¡æŒ¡15%çš„å•è¯
    - ä»»åŠ¡2-é¢„æµ‹ä¸‹ä¸€ä¸ªå¥å­ï¼ˆPredict the Next Sentenceï¼‰ï¼š æŠŠä¸¤ä¸ªå¥å­æ”¾åœ¨ä¸€èµ·ï¼Œè®©encoderåˆ¤æ–­ä¸¤å¥è¯æ˜¯ä¸æ˜¯åŸæ–‡é‡Œé¢ç›¸é‚»çš„ä¸¤å¥è¯ï¼Œæ­£æ ·æœ¬æ‘˜è‡ªåŸæ–‡ï¼Œè´Ÿæ ·æœ¬æ˜¯éšæœºé€‰æ‹©50%ã€‚
- Bertå­¦ä¹ çš„æ—¶å€™ï¼ŒæŠŠä¸Šé¢ä¸¤ä¸ªä»»åŠ¡ç»“åˆèµ·æ¥
- å‡å¦‚æœ‰ä¸¤ä¸ªè¯è¢«é®æŒ¡ï¼Œå°±è¦è®­ç»ƒä¸‰ä¸ªä»»åŠ¡ï¼Œ2ä¸ªé¢„æµ‹é®æŒ¡è¯ä»»åŠ¡ï¼Œ1ä¸ªé¢„æµ‹æ˜¯å¦é‚»è¿‘çš„ä»»åŠ¡ã€‚å‰é¢ä¸¤ä¸ªæ˜¯ä¸€ä¸ªmulti-classåˆ†ç±»ï¼›åé¢ä¸€ä¸ªæ˜¯ä¸€ä¸ªäºŒåˆ†ç±»ã€‚å‰ä¸¤ä¸ªä»»åŠ¡çš„æŸå¤±å‡½æ•°æ˜¯cross entropyï¼Œç¬¬ä¸‰ä¸ªä»»åŠ¡çš„æŸå¤±å‡½æ•°æ˜¯binary-entropyã€‚
- æœ€ç»ˆçš„ç›®æ ‡å‡½æ•°ï¼Œæ˜¯ä¸Šé¢ä¸‰ä¸ªæŸå¤±å‡½æ•°çš„æ±‚å’Œï¼ŒæŠŠæœ€ç»ˆçš„ç›®æ ‡å‡½æ•°å…³äºæ¨¡å‹å‚æ•°æ±‚æ¢¯åº¦ï¼Œç„¶åä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ±‚å‚æ•°ã€‚
- Bertçš„ä¼˜ç‚¹ï¼šä¸éœ€è¦äººå·¥æ ‡æ³¨æ•°æ®ï¼Œè®­ç»ƒæ•°æ®å¯ä»¥ä»wiki/ç½‘é¡µç­‰ï¼Œé•¿åº¦ä¸º2.5billionå•è¯
- Bertå¯ä»¥åˆ©ç”¨æµ·é‡æ•°æ®è®­ç»ƒä¸€ä¸ªè¶…çº§å¤§çš„æ¨¡å‹
- Bertæƒ³æ³•ç®€å•æœ‰æ•ˆï¼Œè®¡ç®—å¤§å®¶å¤§ï¼Œbertæœ‰ä¸¤ä¸ªç‰ˆæœ¬
  - baseï¼š 1.1yiå‚æ•°, 16ä¸ªtpuè®­ç»ƒ4daysï¼Œä¸ç®—è°ƒå‚æ•°ï¼Œè¯¥å‚æ•°æ˜¯å…¬å¼€çš„
  - largeï¼š2.35yiå‚æ•°ï¼Œ64ä¸ªtpuè®­ç»ƒ4daysï¼Œä¸ç®—è°ƒå‚æ•°ï¼Œè¯¥å‚æ•°æ˜¯å…¬å¼€çš„
- æƒ³ç”¨transformerç›´æ¥ä¸‹è½½å¾è®­ç»ƒå¥½çš„bertæ¨¡å‹å°±å¥½ï¼Œæ‹¿åˆ°å‚æ•°å°±å¯ä»¥å¯¹è‹±æ–‡ç¼–ç äº†
- RoBERTaå»ºè®®åªç”¨maskingï¼Œè€Œä¸”æ˜¯åŠ¨æ€masking,


# é™„å½•

## bertæ¨¡å‹åŸç†

![bertæ¨¡å‹å¯è§†åŒ–](https://images.prismic.io/peltarionv2/e69c6ec6-50d9-43e9-96f0-a09bb338199f_BERT_model.png?auto=compress%2Cformat&rect=0%2C0%2C2668%2C3126&w=1980&h=2320)

### ä»»åŠ¡ä¸€--é¢„æµ‹é®æŒ¡è¯

ä»»åŠ¡å¯ä»¥æè¿°ä¸ºï¼š
è¾“å…¥ï¼š â€œThe _____ sat on the matâ€
è¾“å‡ºï¼š What is the masked word?

![img.png](./img.png)

å¦‚ä½•å­¦ä¹ ï¼Ÿ

- eï¼šone-hot vector of the masked word â€œcatâ€.
- ğ©: output probability distribution at the masked position.
- æŸå¤±å‡½æ•°Loss = CrossEntropy(ğ, ğ© )
- â€¢ Performing one gradient descent to update the model parameters.

### ä»»åŠ¡äºŒ-- Predict the Next Sentence

ä»»åŠ¡å¯ä»¥æè¿°ä¸ºï¼š

â€¢ Given the sentence:
â€œcalculus is a branch of mathâ€.
â€¢ Is this the next sentence?
â€œit was developed by newton and leibnizâ€
å¯ä»¥è¡¨è¿°ä¸ºä¸€ä¸ªè€Œåˆ†ç±»é—®é¢˜
â€¢ è¾“å…¥:
[CLS] â€œcalculus is a branch of mathâ€
[SEP] â€œit was developed by newton and leibnizâ€ 
â€¢ Target: true

â€¢ [CLS] is a token for classification.
â€¢ [SEP] is for separating sentences.

å­¦ä¹ è¿‡ç¨‹

![img_1.png](./img_1.png)


### ç»“åˆä¸¤ä¸ªä»»åŠ¡

â€¢ Input:
â€œ[CLS] calculus is a [MASK] of math
[SEP] it [MASK] developed by newton and leibnizâ€.

â€¢ Targets: true, â€œbranchâ€, â€œwasâ€.


## Bertå®è·µã€todoã€‘

å¦‚ä½•ä½¿ç”¨BERTåšè¿ç§»å­¦ä¹ ï¼ˆTransfer Learningï¼‰ï¼Ÿ

- (demoæ•°æ®)[https://www.kaggle.com/c/fake-news-pair-classification-challenge/data]
- é¢„è®­ç»ƒçš„ä¸­æ–‡bertæ¨¡å‹ï¼šhugging face

## è¦ä¸è¦å†»ç»“BERTçš„éƒ¨åˆ†å‚æ•°ï¼Ÿ

Why not to freeze BERTï¼Ÿ

That said, BERT is meant to be fine-tuned. The paper talks about the feature-based (i.e., frozen) vs fine-tuning approaches in more general terms, but the module doc spells it out clearly: "fine-tuning all parameters is the recommended practice." This gives the final parts of computing the pooled output a better shot at adapting to the features that matter most for the task at hand.

when you fine-tune BERT, you can choose whether to freeze the BERT layers or not. Do you want BERT to learn to embed the words in a slightly different way, based on your new data, or do you just want to learn to classify the texts in a new way (with the standard BERT embedding of the words)?

I wanted to use BertViz visualisation to see what effect the classification tuning had on the attention heads, so I did fine-tuning with the first 8 layers of BERT frozen and the remaining 4 layers unfrozen.

Some people suggest doing gradual unfreezing of the BERT layers, ie finetuning with BERT frozen, then finetuning a bit more with just one layer unfrozen, etc.





# å‚è€ƒ
1. Bahdanau, Cho, & Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.
2. Cheng, Dong, & Lapata. Long Short-Term Memory-Networks for Machine Reading. In EMNLP, 2016.
3. Vaswani et al. Attention Is All You Need. In NIPS, 2017.
4. [BERT (é¢„è®­ç»ƒTransformeræ¨¡å‹)](https://www.youtube.com/watch?v=UlC6AjQWao8&t=26s)
5. [RoBERTa](https://arxiv.org/pdf/1907.11692v1.pdf)
6. Devlin, Chang, Lee, and Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In ACL, 2019.
7. [attack_on_bert_transfer_learning_in_nlp-blog](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)
8. [ROBERTA-pytorch](https://pytorch.org/hub/pytorch_fairseq_roberta/)
9. [huggingface](https://discuss.huggingface.co/t/fine-tune-bert-models/1554/2)
10. https://medium.com/swlh/painless-fine-tuning-of-bert-in-pytorch-b91c14912caa