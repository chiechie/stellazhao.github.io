---
title:  chapter 7 深度学习的第一性原理
author: chiechie
mathjax: true
date: 2021-05-01 18:39:48
tags: 
- 人工智能
- 深度学习
- 深度学习原理
- AGI
- high-level
categories: 
- 机器学习
---




## 学术界的进展

1. 都知道图像数据可能更适合cnn，序列数据更适合GPT-3，这些网络都是具有某个特殊结构的DNN，这些奇奇怪怪的结构是无数次试错，人工设计出来的。那么，有没有可能存在一种通用的网络结构或者网络结构的设计方法呢？即，不管是对图像数据，还是序列数据，更甚至，不管是有监督还是无监督，都能找到统一的模型训练方法呢？(这个问题很宏大，涵盖了NAS)
2. 针对这个问题，学术界已有的研究方向主要是从信息论角度出发，有两项工作比较知名，一个是受Hinton推崇的“信息瓶颈”（Information bottleneck）理论，一个是马老师团队提出的MCR^2理论。以x，z，y 分别表示输入数据，学到的特征以及标签：

    - **信息瓶颈**要解决有监督问题的目标是最大化z和y之间的互信息，而最小化x和z之间的互信息。
    - **MCR^2**则一方面在最大化x和z之间的互信息，也在最大化z和y之间的互信息。
    - **信息瓶颈**比较极端，既然我们的目标是分类，那就直奔主题，它喜欢的特征就是对分类有效的特征，至于这个特征对最原始的输入的表示来说是不是好就不关心了，甚至要尽可能删除与分类无关的信息。
    - **MCR^2**的目标函数由两部分构成，第一部分与类别无关，实际上和InfoMax等价（InfoMax直接计算熵有缺陷，正如马老师论文提到的，熵在一些退化情况下没有良好的定义，所以他们用了率失真函数）。第二部分和类别相关，和信息瓶颈的目标一致。
1. 有监督or无监督？判别模型or生成模型？半监督学习有没有必要？为什么不带标签的数据对训练分类模型也有帮助呢？目前比较一致的结论是，无标签数据相当于提供了一种正则化（regularization）,有助于更准确地学习到输入数据所分布的流形（manifold)，而这个低维流形就是数据的本质表示，它对分类有帮助。
6. 但这又带来一个问题：为什么数据的本质规律对预测它的标签有帮助？
7. 这又带来一个更本质的问题：数据的标签是怎么来的？
8. 这些问题，和中国古代哲学里的“名实之论”有关系。到底是先有“名”，还是先有“实”。一般认为是先有“实”，后有“名”，而“实”决定“名”。
9. 回到机器学习问题上，就是数据的标签是后天出现的，一个东西到底用什么标签去称呼，本质上是由这个东西自身的规律决定的。换句话说，label可能是后天“涌现”（emerge)的，因为一些东西本质一样，长的像，所以人们才会给他们相同的命名。因此要寻求最优的分类器，可能首先要从“无监督”的数据分析入手。
10. 基于无标注的语料可以训练出来和几年前有监督学习的SOTA模型相匹配的结果。可以期待，无监督学习未来会出现更多的成绩。



## 附录

1. Ideal Intelligence或者AGI是什么？就是在做信号压缩，即从大量数据中找到所有的模式。
2. 数据压缩就是describe the large amout of data in a more compact way
3. finding all patterns = short description of raw data（low kolmogorov complexity）
1. 用压缩的视角看clustering：聚成两类后是否比原来散开，占用空间小。
2. 对混合分布的数据进行分类，传统方法基于最大化后验概率（MAP），难点在于后验概率很难estimate和定义，当分布退化时。(所以svm和deep networks赢了)
3. 用压缩的角度看分类：记录每个类别中对每个样本编码的比特数，分类的原则--Minimum Incremental Coding Length (MICL)
2. 回看discriminative model和generative model之辩，前者直接学习用户关心的条件概率p(y|x)，但后者需要学习p(x|y)p(y)，也就是还要学习输入特征的概率分布。发明SVM的Vapnik 认为generative model做了一些和目标无关的、多余的事情，因此而更推崇前者，甚至抛出了奥卡姆剃刀“如无必要，勿增实体”。
6. 信息瓶颈的论文里论述了这样一个结论:特征条件独立，最优分类器就是线性的。此外，Andrew Ng在论述Naive bayes和Logistic regression等价关系时讨论过.


### 计算神经科学

1. 计算神经科学领域对这个问题已经研究很多年了，有一些非常经典的研究成果譬如ICA(独立成分分析）以及Sparse coding方法，这些方法的背后是一种基于信息论的原则，称之为efficient coding principle。
2. 这个理论认为，大脑的结构源自亿万年的进化，进化的目标是形成外界物理环境的一种“最经济”的表达，这种表达是适应于（adapt to)自然界的统计规律，而且这种结构基本上是无监督的。这种理论已经能非常好的解释视网膜、侧膝体、初级视皮层神经元感受野的形成机理，近些年的研究开始向理解V2, V4等更高级的视皮层进发。
3. 用efficient coding原则来理解卷积神经网络的一些关键技巧
  
    - 卷积: 在计算神经科学里，相当于初级视皮层神经元的局部感受野
    - 非线性的激活函数：计算神经科学里，站在信息论角度，通过非线性映射，可以把取值范围很大的activation映射到一个区间，比较重要的输入值编码的分辨率高一些，而不重要的输入不需要消耗太多能量去编码，就被映射到“饱和区”。
4. efficient coding准则认为，神经元的感受野是用来表示输入刺激的统计规律，把输入转化成神经元的响应有助于去除输入之间的冗余性（redudancy）,也就是神经元的响应之间应该比输入 “统计上更接近于独立”。


### lecake

a cherry refers to the amount of data your getting，the information youare getting in
reinforcement learning，the reward are not a very high throuput signal you you
are getting。


总结一下lecake的观点：以非监督学习为基石，辅以大量场景的监督学习或者少量场景的纯强化学习，是走向AGI的正确路子

![lecake](./db22b6d4fc545430dce3009785f84b21.png)

## 工业界的进展

1. 项研究和今天很受关注的大规模预训练模型也很相似，而且也有研究发现，大规模预训练模型中真正有效的实际上是一个很小的“子网络”，也就是很多连接的权重是接近零的。最近恰好出现了一些 MLP is all your need相关的研究，也不无道理。
2. 最近除了自然语言处理，在图像和语音领域，无监督和自监督学习也取得了很大进展，譬如Facebook 的wave2vec 

## 参考文献
1. [Deep_Networks_from_First_Principles](https://cmsa.fas.harvard.edu/wp-content/uploads/2021/04/Deep_Networks_from_First_Principles.pdf)
2. [我对深度学习“第一性原理”的探索和理解
-袁进辉](https://mp.weixin.qq.com/s/no0u_6m3Ima8YlmV7msGqQ)
3. https://www.youtube.com/watch?v=V9Roouqfu-M