---
title: 机器学习基础
author: chiechie
mathjax: true
date: 2021-04-15 16:00:28
tags:
- 机器学习
- 人工智能
categories:
- 机器学习
---



## 机器学习的基本概念

1. 机器学习是一个多领域交叉的学科，但是总结起来，关注的核心点都在于：如何从历史经验（数据）中获取知识或者规律（模型），并将知识应用到新的类似的场景中。

2. 机器学习的三个要素：假设函数空间；学习策略or目标函数；优化算法。

3. 机器学习的流程一般要经历几个阶段：收集样本（大部分场景都要人工标注），特征提取，定义预测函数（也叫假设函数空间，线性的还是非线性的），定义损失函数/目标函数，选择优化算法。

   > 使用sklearn的时候，后面三步骤写一行命令就完成了。

   - 定义损失函数：给定预测模型之后，让他去预测训练数据中的每一个输入样本，为了让预测模型学的越来越准，我们需要设置一个奖惩机制：当模型预测对了，就给一个高分，预测错了就给一个低分。负责对预测函数打分的组建就是这个损失函数，当然，他只会惩罚错误的预测结果，即，预测错了就跟一个很严重的惩罚分数，这样我们的目的就变成，希望找到一个惩罚分数最低的预测函数。怎么找呢？这个就涉及到最优化的知识，当损失函数是个连续可导的函数，更甚至是一个凸函数时，比如误差平方和交叉熵函数，我们就可以利用导数的方法来求最优的参数
   - 选择优化方法：当有了损失函数和训练数据之后，我们就可以定义一个优化问题--最小化所有训练数据的总体预测误差，通常也称之为经验风险，通过合适的优化方法去求解这个最优化问题，就可以得到最佳的预测函数。优化方法有很多，比如梯度下降，牛顿法等，能否找到全局最优解，取决于损失函数的复杂程度，如果损失函数是一个凸函数，很容易找到全局最优，否则，大部分情况都只能找到局部最优。

## 机器学习的细分方向

从预测函数的类别来看，可以分为回归，分类，排序，有结构的预测

从复杂度来看，可以分为线性和非线性；浅层和深度

- 线性：线性回归/逻辑回归，线性支持向量机。
- 非线性：树模型，深度神经网络（全链接神经网络，卷积神经网络，循环神经网络）

从模型功能，可以分为生成模型和判别模型

- 生成模型的学习目标是mle，关注特征和标签的联合概率，它要学习的概率分布很复杂，但是适用场景多，eg分类，概率密度估计，样本生成。
- 判别模型的学习目标是最大化条件概率或者后验概率，不关注特征和标签的联合概率，直接对后验概率建模，学习效率高，不过只能做分类。

###  常用的机器学习模型

### 线性模型/逻辑回归

1. 线性模型是最简单的机器学习模型，这个模型假设预测结果是特征的线性组合。

2. 线性模型可以预测连续变量，如果要预测离散变量呢？需要将特征的线性组合结果使用对数几率函数进行变换 ，因为其值域是0～1，所以一般用来对概率建模，基于概率的预测结果，配合对数似然损失或者交叉熵损失，可以定义训练目标。

   $$g(x ; w)=\frac{1}{1+\exp \left(-w^{T} x\right)}$$

3. 线性模型可解释性强，但是不能表达特征之间复杂的非线性组合关系。所以，一般在使用线性摸模型之前，会配合复杂的特征工程：比如单特征取指数，对数，多项式变换，以及特征之间交叉乘积，或者使用核方法，将原特征隐式映射到一个高维的非线性空间，然后在高维空间里面构建线性模型，eg 支持向量机。

### 支持向量机

1. 核方法的基本思想，通过一个非线性变换，把输入数据映射到高维度的希尔伯特空间，在高维空间中，原先线性不可分的问题，变得很好解决。

2. 使用核方法时，理论上，计算高维空间中两个向量的距离要分两步走：1.先将输入向量映射到高维空间，2. 然后再计算高维空间中向量之间的距离，但是，在已知$\phi$的表达式时，是可以直接推导出低维空间中两个向量的距离函数$k(x_1, x_2)$

$x_1$--> $\phi(x_1)$

$x_2$--> $\phi(x_2)$

$dist(\phi(x_1), \phi(x_2)) = k(x_1, x_2)$

3. 举几个例子核函数的例子：

- 自由度为d的多项式核函数:$k(x,y) = (x^\mathsf{T} y + c)^{d}$

>  假设d=2对应的映射函数：$ \varphi(x) = \langle x_n^2, \ldots, x_1^2, \sqrt{2} x_n x_{n-1}, \ldots, \sqrt{2} x_n x_1, \sqrt{2} x_{n-1} x_{n-2}, \ldots, \sqrt{2} x_{n-1} x_{1}, \ldots, \sqrt{2} x_{2} x_{1}, \sqrt{2c} x_n, \ldots, \sqrt{2c} x_1, c \rangle $

- RBF核函数或者高斯核函数：$k(\mathbf{x}, \mathbf{y})=e^{-\gamma\|\mathbf{x}-\mathbf{y}\|^{2}}, \gamma>0$

- sigmoid核函数: $\tanh \left(\gamma\left\langle x, x^{\prime}\right\rangle+r\right)$

> 注意，核函数中的参数如$\gamma$是需要人工设定的，所以最好做归一化

4. 核方法的典型的应用之一就是支持向量机。支持向量机的基本思想是，通过核函数将原始输入映射为一个高维向量，在高维度空间里面寻找一个超平面，将正负样本分开，即寻找一个最大间隔的超平面。

   $$\min \hat{l}_{n}(w)=\frac{1}{2}\left\|_{w}\right\|^{2}+\sum\limits_{i=1}^{n} \max \left\{0,1-y_{i}\left(w^{T} \phi\left(x_{i}\right)\right)\right\}$$

   使用拉格朗日乘子法，转化为求其对偶问题：

   $$\max _{\alpha} \sum_{i=1}^{n} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_{i} \alpha_{j} y_{i} y_{j} \phi\left(x_{i}\right)^{T} \phi\left(x_{j}\right)$$

   这是一个二次规划问题。

5. 核函数的选择：需满足核矩阵(n*n)半正定，这样总能找到隐含的映射$\phi$

   >  一个核矩阵隐式定义了一个映射$\phi$，称之为再生核希尔伯特空间（reproducing kernel hilvert space, RKHS）

### 树模型

1. 决策树也是一个常见的机器学习模型，基本思想是，根据特征构造出树结构的预测模型
2. 一个决策树包含一个根节点，若干内部节点以及叶子节点。
3. 叶子节点对应最终的决策结果，其他节点针对数据的某个属性进行判断和分支：在该节点中，会根据数据的某个特征的值进行判断，一局判断结果将样本划分到某个子树之中，
4. 通过决策树，可以从根节点出发，把一个具体的样本最终分配到某个叶子节点上，实现预测的目的。
5. 因为每个节点上的分支操作是非线性的，因此决策树可以实现比较复杂的非线性映射，
6. 决策树算法的目的是，根据训练数据，学习处一颗泛化能力较强的树，能够很精准的把位置样本分到对应的叶子节点上。
7. 树模型的优势在于可解释性，但是很难兼顾准确性和泛化性，
8. 常见的决策树算法包括：分类回归树（CART），ID3算法，C4.5算法，决策树桩（Decision Stump）。总结起来，这些算法的流程都包括分支和剪枝两个步骤。
9. 分支的目的是，找到一个划分条件（一般是某个特征满足某个条件），使得划分之后能够提升样本的纯度。
10. 剪枝要解决的问题是减少过拟合，有两种剪枝的技术：预剪枝和后剪枝。
    1. 预剪枝是指，在决策树生成过程中，对每个节点在划分前使用验证集来估计，该划分能否带来决策树泛化性能提升，如果没有提升则将当前节点标记为叶子结点；
    2. 后剪枝是指，从训练集中生成一颗完整的决策树，然后自下而上的考察去掉每个节点是否提升了泛化能力（将该节点及其子树合并成一个叶子节点），没有提升的话就进行剪枝。

11. 有些情况下数据中隐含的模式比较复杂，那么单个决策树的表达性能不够，通常会使用集成学习来提升树的表达能力。将多棵树进行组合，有不同的方式组合，如bagging/boosting等。
12. adaboosting的思路是，先训练出一个base estimator，根据预测结果的准确率，对样本的权重进行调整，预测不准的样本调高权重，预测准的样本调低权重，然后让下一个base estimmtor来学习。这样，原来base estimator搞不定的样本，在后续的学习中，得到更多的关注，最终的预测模型，是所有base estimator预测结果的线性组合，权重表示对应base estimator的预测准确率。
13. boosting在抵抗过拟合方面效果很好，随着训练的推进的，即使在训练机已经把误差降到0，更多的迭代还可以提高测试机上的准确率，可以用间隔定理（Margine Theory）来解释这个现象：随着迭代的推进，训练样本上的分类置信度（即每个样本点上的间隔）仍在不断变大。



### 神经网络

1. 神经网络是一类典型的非线性模型，它的结构设计受到生物神经网络的启发。对大脑生理机制研究发现，其基本单元是神经元，每个神经元通过树突从上游的神经元那里获取输入信号，经过自身加工处理，当能量达到一定强度，就会激活产生一个新的信号，并将这个信号通过轴突传递给下游神经元。
2. 设计人工神经网络时，并没有参考生物系统的激活机制，因为生物系统的激活机制相当于是阶跃函数，这个函数不连续，因此不能使用基于梯度的方法来更新参数。取而代之的是更平滑的激活函数，比如sigmoid函数，双曲正切函数（tanh），relu。

![dl0_basic](./dl0_basic.png)

3. 全连接神经网络：

4. 卷积神经网络：研究发现每个视觉细胞只对图像中的局部区域敏感，大量视觉细胞平铺开来，可以利用图像的空间局部上的相关性，受到生物系统的启发，卷积神经网络也引进了局部连接的概念，并且在空间上平铺多个卷积核/滤波器，这些卷积核之间有很大的重叠区域，相当有个滑动窗口，当窗口滑倒不同空间位置时，对窗口内的信息使用同样的滤波器进行分析。经过卷积操作之后，会得到一个和原图像类似大小的新图层，其中的每个点都是卷积核在局部区域的作用结果，对应于更加高级的语义信息，得到的新图层也叫特征映射（feature map），对于一副图，可以在一个卷积层里使用多个不同的卷积核，从而得到多维的feature map；也可以把多个卷积层stack起来，不断抽取越来越高级的语义信息。除了卷积，池化也是卷积神经网络的重要部分，池化的目的是，对特征映射进行压缩，体现平移不变性，并且有效扩大后续卷机操作的感受野。池化没有参数，只是对局部区域求一下汇总信息，如平均值/中位数/最大值/最小值。

5. 实际中，设计cnn时，经常把卷积层和池化层交易级联，从而实现不断抽取高层语义特征的目的，最后一层可以加上一个全连接层，对高层的语义特征进行预测。

   ![image-20210720201740269](./image-20210720201740269.png)

6. 人们在imagenet数据集上不断通过增加网络深度刷新错误率，2015年来自微软的152层的Resnet网络，在Image数据机上取得了3.75%的top5的错误率。

7. 随着卷积神经网络变得越来越深，梯度消失的问题越来越严重，给模型的训练带来了很大的难度，为了解决这个问题，提出了许多新的方法，比如残差学习，高密度网络，实验表明：这些方法可以有效把训练误差传递到靠近输入层的地方。![image-20210720204128001](./image-20210720204128001.png)

循环神经网络

1. 循环神经网络也有很强的仿生学基础，人类是如何阅读看报呢？每读到一个词，都会想一想刚刚读到的文字在脑袋中形成的记忆，记忆能够帮我们更好地理解当前看到的文字，当看下一个文字时，当前文字和历史记忆又会共同成为新的记忆，并对理解下一个文字提供帮助。

2. 循环神经网络的设计也是参考了这个思想，用$s_t$表示t时刻的记忆，是由t时刻看到的输入$x_t$和t-1时刻的记忆$s_{t-1}$共同作用产生的，这个过程可以表示为：

   $s_t = \Psi(Ux_t + W s_{t-1})$, 

   $o_t = V s_t$

3. 这个式子蕴含着对于记忆单元$s_t$的循环迭代，在实际应用中，无现场时间的循环迭代是没有意义的，比如我们阅读文字时，每个句子的平均长度可能只有十几个词，因此，可以把循环神经网络在时域上展开，然后在展开的网络上利用梯度下降来更新参数矩阵UVW，称之为时域反向传播（Back Propagation Through Time，BPTT）

4. 和全连接神经网络，卷积神经网络类似，循环神经网络时间域展开后，也会遇到梯度消失的问题，为了解决这个问题，提出了一套依靠控制门来控制信息流通的方法。也就是说，在循环神经网络的两层之间，同时存在线性和非线性通路，哪个通路开/关，开多大程度？取决于一组控制们，这个控制门是带参数的，两种使用较多的方法是LSTM和GRU。GRU更简单些，，LSTM有3个控制门，GRU有两个控制们，二者在实际中的效果类似，但是GRU的训练速度要快些。

5. 在使用基于循环神经网络的seq2seq模型做机器翻译时，实践中会遇到一个问题，输出端的翻译结果中，某个词是对于输入端每个词语依赖程度不一样，通过吧整个输入句子编码到一个向量来驱动输出的句子，会导致信息粒度太粗糙，或者长期的依赖关系被忽视。为了解决这个问题，在标准的seq2seq基础上，引入了“注意力机制”，输出端每次输出之前，都会重新回归一下输入的每个单词，并且会计算每个单词的重要性，从而具备更细粒度的表达能力。



## 常用的优化方法



1. 远在机器学习之前，梯度下降就已经被提出，还有共轭梯度法，坐标下降法，牛顿法，拟牛顿法，Frank-Wolfe方法，Nesterov加速方法，内点法，对偶方法他确定性优化算法。
2. 确定性优化算法从算法使用的信息的角度可以分为一阶和二阶方法，
   - 一阶：只用到了目标函数的一阶导数信息
   - 二阶：用到了二阶导数，如嗨森矩阵
3. 随着大数据的兴起，确定性优化算法的效率成为瓶颈。为了减少优化过程中每次迭代的计算复杂度，开始关注随机优化算法，比如随机梯度下降，随机坐标下降等。这些算法的基本思想是，每次迭代不使用全部样本或者全部特征，而是随机抽样一个/或者一组样本，或者一个/一组特征，再利用这些抽象的信息来计算一阶或者二阶导数，对目标函数进行优化。很多情况下，随机优化可以看成是确定性算法的无偏估计，但是，有时候也会带来较大方差，需要使用新的技术手段去控制方差，比如SVRG。
4. 上面的优化算法在求解凸优化问题时，理论性质是比较清楚的，但是深度神经网络引入了非凸优化问题，以上的算法在该问题域还存在很大的理论空白，例如：
   1. 优化算法的收敛性在非凸问题能否保持
   2. 有没有什么加速的方法？
5. 专门针对深度神经网络的优化算法被发明，例如，带冲量的速记梯度下降，ada梯度，rmsprop，ada delta

  - 带动量的随机梯度下降，对所有历史的梯度，按照时间衰减为权重，一起决定下一步的更新方向
  - ada梯度：在上面的基础上，还考虑了根据历史梯度大小调整了学习步长
  - rmsprop：带动量的sgd和ada的结合
  - ada delta：在rmsprop基础行，进一步对步长调整
  - adam：





### 机器学习理论

机器学习最关注的是泛化能力，反映了机器能否抓住问题本质，获得处理未知样本的能力



1. 机器学习算法的最终目标是最小化损失的期望，但是我们不知道真实分布，所以将目标转为，最小化经验风险，同时，会给函数空间添加适当的约束，比如只允许搜索那些范数小于c的函数子空间，成学习问题为正则经验风险最小化，我们希望学习到的模型的期望风险尽可能小，并将其定义为机器学习的泛化误差。
2. 希望算法的泛化误差尽可能小，即，算法输出的模型和最优的模型对应的期望风险之差尽可能小，这个差距也称为泛化误差，可以分解为三部分：
   1. 优化误差：迭代出来的算法与精确的最小经验风险的模型的差别，这个误差是由优化算法的局限性带来的，与选用的优化算法，数据量大小，迭代轮数，以及函数空间有关。
   2. 估计误差：最小经验风险模型和最小期望风险模型所对应的差别。
   3. 近似误差：与函数空间的表达能力有关
3. 基于容度的估计误差的上界：估计误差可以被函数集合中的所有函数的经验风险和期望风险的一致上界控制，通常被称为一致估计偏差。一致估计偏差，被函数集合G的容度控制住，容度描述了函数G在多个输入数据上所产生的输出的多样性，比如VC维告诉我们函数集合最多能打伞多少个样本点，。




## 附录



## 基本概念

- 信息增益: 衡量切分前后，样本纯度的提升or混乱度的下降。

```python
IG = information before splitting (parent) — information after splitting (children)
```

- 具体的，有两个衡量纯度/混乱度的指标：Entropy 和 Gini Impurity
  - 基尼系数（**gini index**）: $$I_{G}=1-\sum_{j=1}^{c} p_{j}^{2}$$
    - $p_j$: 落入该节点的样本中，第j类样本的占比
    - 如果所有样本都属于某一类c，gini系数最小，为0。
  - 熵（entropy）：$$I_{H}=-\sum_{j=1}^{c} p_{j} \log _{2}\left(p_{j}\right)$$
    - $p_j$: 落入该节点的样本中，第j类样本的占比
    - 如果所有样本都属于某一类c，熵最小，为0。

## 参考

1. 分布式机器学习
2. [sklearn-kernel](https://scikit-learn.org/stable/modules/svm.html#svm-kernels)
