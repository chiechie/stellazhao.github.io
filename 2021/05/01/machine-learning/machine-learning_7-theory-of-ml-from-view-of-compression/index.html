<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chiechie.github.io","root":"/","images":"/images","scheme":"Gemini","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="最近一篇工业界的人讲深度学习的第一性原理，结合mayi老师的文章，尤其是联想到中国哲学中的&quot;名实之争&quot;，有点意思，在这里记录一下  summary  Ideal Intelligence或者AGI是什么？就是在做信号压缩，即从大量数据中找到所有的模式。 数据压缩就是describe the large amout of data in a more compact way finding all">
<meta property="og:type" content="article">
<meta property="og:title" content="chapter 7 机器学习理论的统一框架-基于压缩视角">
<meta property="og:url" content="https://chiechie.github.io/2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/index.html">
<meta property="og:site_name" content="Chiechie&#39;s Mini World">
<meta property="og:description" content="最近一篇工业界的人讲深度学习的第一性原理，结合mayi老师的文章，尤其是联想到中国哲学中的&quot;名实之争&quot;，有点意思，在这里记录一下  summary  Ideal Intelligence或者AGI是什么？就是在做信号压缩，即从大量数据中找到所有的模式。 数据压缩就是describe the large amout of data in a more compact way finding all">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chiechie.github.io/2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/db22b6d4fc545430dce3009785f84b21.png">
<meta property="article:published_time" content="2021-05-01T10:39:48.000Z">
<meta property="article:modified_time" content="2021-07-22T07:58:57.995Z">
<meta property="article:author" content="Chiechie">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="深度学习原理">
<meta property="article:tag" content="AGI">
<meta property="article:tag" content="high-level">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chiechie.github.io/2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/db22b6d4fc545430dce3009785f84b21.png">


<link rel="canonical" href="https://chiechie.github.io/2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://chiechie.github.io/2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/","path":"2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/","title":"chapter 7 机器学习理论的统一框架-基于压缩视角"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>chapter 7 机器学习理论的统一框架-基于压缩视角 | Chiechie's Mini World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="Chiechie's Mini World" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chiechie's Mini World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">在我们离开的时候，这个世界还是那么愚蠢和邪恶，跟我们刚的时候并无二致。---伏尔泰</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#summary"><span class="nav-number">1.</span> <span class="nav-text">summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E6%9C%AF%E7%95%8C%E7%9A%84%E8%BF%9B%E5%B1%95"><span class="nav-number">2.</span> <span class="nav-text">学术界的进展</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%A7%91%E5%AD%A6"><span class="nav-number">2.1.</span> <span class="nav-text">计算神经科学</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E7%93%B6%E9%A2%88-%E5%92%8C-mcr2"><span class="nav-number">2.2.</span> <span class="nav-text">信息瓶颈 和 MCR^2</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mcr2%E7%BB%86%E8%8A%82"><span class="nav-number">2.3.</span> <span class="nav-text">MCR^2细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E7%9B%91%E7%9D%A3-vs-%E6%97%A0%E7%9B%91%E7%9D%A3%E4%B9%8B%E4%BA%89"><span class="nav-number">2.4.</span> <span class="nav-text">有监督 vs 无监督之争</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lecake"><span class="nav-number">2.5.</span> <span class="nav-text">lecake</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B7%A5%E4%B8%9A%E7%95%8C%E7%9A%84%E8%BF%9B%E5%B1%95"><span class="nav-number">3.</span> <span class="nav-text">工业界的进展</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-number">4.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chiechie</p>
  <div class="site-description" itemprop="description">a reader & thinker</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">182</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">205</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/05/01/machine-learning/machine-learning_7-theory-of-ml-from-view-of-compression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chapter 7 机器学习理论的统一框架-基于压缩视角
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-05-01 18:39:48" itemprop="dateCreated datePublished" datetime="2021-05-01T18:39:48+08:00">2021-05-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-07-22 15:58:57" itemprop="dateModified" datetime="2021-07-22T15:58:57+08:00">2021-07-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>最近一篇工业界的人讲深度学习的第一性原理，结合mayi老师的文章，尤其是联想到中国哲学中的"名实之争"，有点意思，在这里记录一下</p>
</blockquote>
<h2 id="summary">summary</h2>
<ol type="1">
<li><p>Ideal Intelligence或者AGI是什么？就是在做信号压缩，即从大量数据中找到所有的模式。</p></li>
<li><p>数据压缩就是describe the large amout of data in a more compact way</p></li>
<li><p>finding all patterns = short description of raw data（low kolmogorov complexity）</p></li>
</ol>
<h2 id="学术界的进展">学术界的进展</h2>
<p>马毅老师的文章和袁进辉文章都是想探索一种通用的方法，对任何数据找到其最优的神经网络。</p>
<p>都知道图像数据可能更适合cnn，序列数据更适合GPT-3，这些网络都是具有某个特殊的结构的DNN，这些奇奇怪怪的结构是无数次试错，人工设计出来的。</p>
<p>那么，有没有可能有一种脱离人的设计方法呢？即，不管是对图像数据，还是序列数据，统一的模型训练方法呢？更甚至，不管是有监督还是无监督，都能找到统一的模型训练方法呢？</p>
<p>这个问题很宏大，涵盖了NAS这个方向。</p>
<h3 id="计算神经科学">计算神经科学</h3>
<ol type="1">
<li><p>计算神经科学领域对这个问题已经研究很多年了，有一些非常经典的研究成果譬如ICA(独立成分分析）以及Sparse coding方法，这些方法的背后是一种基于信息论的原则，称之为efficient coding principle。</p></li>
<li><p>这个理论认为，大脑的结构源自亿万年的进化，进化的目标是形成外界物理环境的一种“最经济”的表达，这种表达是适应于（adapt to)自然界的统计规律，而且这种结构基本上是无监督的。这种理论已经能非常好的解释视网膜、侧膝体、初级视皮层神经元感受野的形成机理，近些年的研究开始向理解V2, V4等更高级的视皮层进发。</p></li>
<li><p>用efficient coding原则来理解卷积神经网络的一些关键技巧</p>
<ul>
<li>卷积: 在计算神经科学里，相当于初级视皮层神经元的局部感受野</li>
<li>非线性的激活函数：计算神经科学里，站在信息论角度，通过非线性映射，可以把取值范围很大的activation映射到一个区间，比较重要的输入值编码的分辨率高一些，而不重要的输入不需要消耗太多能量去编码，就被映射到“饱和区”。</li>
</ul></li>
<li><p>efficient coding准则认为，神经元的感受野是用来表示输入刺激的统计规律，把输入转化成神经元的响应有助于去除输入之间的冗余性（redudancy）,也就是神经元的响应之间应该比输入 “统计上更接近于独立”。</p></li>
</ol>
<h3 id="信息瓶颈-和-mcr2">信息瓶颈 和 MCR^2</h3>
<p>针对这个问题，学术界已有的研究方向主要是从信息论角度出发，有两项工作比较知名，一个是受Hinton推崇的“信息瓶颈”（Information bottleneck）理论，一个是马老师团队提出的MCR^2理论。</p>
<p>以x，z，y 分别表示输入数据，学到的特征以及标签：</p>
<ul>
<li>信息瓶颈要解决有监督问题的目标是最大化z和y之间的互信息，而最小化x和z之间的互信息。</li>
<li>MCR^2则一方面在最大化x和z之间的互信息，也在最大化z和y之间的互信息。</li>
</ul>
<p>信息瓶颈那篇论文里论述了这样一个结论:特征条件独立，最优分类器就是线性的。此外，Andrew Ng在论述Naive bayes和Logistic regression等价关系时讨论过.</p>
<h3 id="mcr2细节">MCR^2细节</h3>
<ol type="1">
<li>用压缩的视角看clustering：聚成两类后是否比原来散开，占用空间小。</li>
<li>对混合分布的数据进行分类，传统方法基于最大化后验概率（MAP），难点在于后验概率很难estimate和定义，当分布退化时。(所以svm和deep networks赢了)</li>
<li>用压缩的角度看分类：记录每个类别中对每个样本编码的比特数，分类的原则--Minimum Incremental Coding Length (MICL)</li>
</ol>
<h3 id="有监督-vs-无监督之争">有监督 vs 无监督之争</h3>
<ol type="1">
<li>有监督or无监督？判别模型or生成模型？半监督学习有没有必要？本质上是在争论一个东西</li>
<li>回看discriminative model和generative model之辩，前者直接学习用户关心的条件概率p(y|x)，但后者需要学习p(x|y)p(y)，也就是还要学习输入特征的概率分布。发明SVM的Vapnik 认为generative model做了一些和目标无关的、多余的事情，因此而更推崇前者，甚至抛出了奥卡姆剃刀“如无必要，勿增实体”。</li>
<li>信息瓶颈处于一个极端，既然我们的目标是分类，那就直奔主题，它喜欢的特征就是对分类有效的特征，至于这个特征对最原始的输入的表示来说是不是好就不关心了，甚至要尽可能删除与分类无关的信息。</li>
<li>MCR^2 的目标函数由两部分构成，第一部分与类别无关，实际上和InfoMax等价（InfoMax直接计算熵有缺陷，正如马老师论文提到的，熵在一些退化情况下没有良好的定义，所以他们用了率失真函数）。第二部分和类别相关，和信息瓶颈的目标一致。</li>
<li>半监督学习到底成不成立？为什么不带标签的数据对训练分类模型也有帮助呢？比较一致的结论是，无标签数据相当于提供了一种正则化（regularization）,有助于更准确地学习到输入数据所分布的流形（manifold)，而这个低维流形就是数据的本质表示，它对分类有帮助。</li>
<li>但这又带来一个问题：为什么数据的本质规律对预测它的标签有帮助？</li>
<li>这又带来一个更本质的问题：数据的标签是怎么来的？</li>
<li>要回答这些问题，就要和中国古代哲学里的”名实之论“有关系了。到底是先有”名“，还是先有”实“。一般认为是先有”实“，后有”名“，而”实“决定”名“。</li>
<li>回到机器学习问题上，就是数据的标签是后天出现的，一个东西到底用什么标签去称呼，本质上是由这个东西自身的规律决定的。换句话说，label可能是后天”涌现“（emerge)的，因为一些东西本质一样，长的像，所以人们才会给他们相同的命名。因此要寻求最优的分类器，可能首先要从”无监督“的数据分析入手。</li>
</ol>
<h3 id="lecake">lecake</h3>
<p>a cherry refers to the amount of data your getting，the information youare getting in reinforcement learning，the reward are not a very high throuput signal you you are getting。</p>
<p>总结一下lecake的观点：以非监督学习为基石，辅以监督学习或者纯强化学习，是走向AFI的方向</p>
<figure>
<img src="./db22b6d4fc545430dce3009785f84b21.png" alt="lecake"><figcaption aria-hidden="true">lecake</figcaption>
</figure>
<h2 id="工业界的进展">工业界的进展</h2>
<ol type="1">
<li>项研究和今天很受关注的大规模预训练模型也很相似，而且也有研究发现，大规模预训练模型中真正有效的实际上是一个很小的“子网络”，也就是很多连接的权重是接近零的。最近恰好出现了一些 MLP is all your need相关的研究，也不无道理。</li>
<li>最近除了自然语言处理，在图像和语音领域，无监督和自监督学习也取得了很大进展，譬如Facebook 的wave2vec 基于无标注的语料可以训练出来和几年前有监督学习的SOTA模型相匹配的结果。可以期待，无监督学习未来会出现更多的成绩。</li>
</ol>
<h2 id="参考文献">参考文献</h2>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://cmsa.fas.harvard.edu/wp-content/uploads/2021/04/Deep_Networks_from_First_Principles.pdf">Deep_Networks_from_First_Principles</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/no0u_6m3Ima8YlmV7msGqQ">我对深度学习“第一性原理”的探索和理解 -袁进辉</a></li>
<li>https://www.youtube.com/watch?v=V9Roouqfu-M</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86/" rel="tag"># 深度学习原理</a>
              <a href="/tags/AGI/" rel="tag"># AGI</a>
              <a href="/tags/high-level/" rel="tag"># high-level</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/30/reinforcement_learning/rl12-monte-calo/" rel="prev" title="强化学习12 蒙特卡洛树搜索">
                  <i class="fa fa-chevron-left"></i> 强化学习12 蒙特卡洛树搜索
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/05/02/zhexue/dao-daodejing1/" rel="next" title="道德经(上) 1-37">
                  道德经(上) 1-37 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiechie</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
