<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chiechie.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="反向传播  ** The problem with Backpropagation is that it is a** leaky abstraction.---Andrej Karpathy   img   关于激活函数--sigmoid，另外一个容易忽视的事实是，其梯度在z&#x3D;0.5时，达到最大--0.25，这意味着，每次梯度信号穿越过sigmoid门时，会衰减为1&#x2F;4或者更多，如果使用基本 S">
<meta property="og:type" content="article">
<meta property="og:title" content="chapter 4.2 backpropagation和训练小技巧">
<meta property="og:url" content="https://chiechie.github.io/2021/04/20/machine-learning/machine-learning_4-dl2_practice/index.html">
<meta property="og:site_name" content="Chiechie&#39;s Mini World">
<meta property="og:description" content="反向传播  ** The problem with Backpropagation is that it is a** leaky abstraction.---Andrej Karpathy   img   关于激活函数--sigmoid，另外一个容易忽视的事实是，其梯度在z&#x3D;0.5时，达到最大--0.25，这意味着，每次梯度信号穿越过sigmoid门时，会衰减为1&#x2F;4或者更多，如果使用基本 S">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://kratzert.github.io/images/bn_backpass/BNcircuit.png">
<meta property="og:image" content="https://miro.medium.com/max/1400/1*gkXI7LYwyGPLU5dn6Jb6Bg.png">
<meta property="og:image" content="https://miro.medium.com/max/1400/1*g0yxlK8kEBw8uA1f82XQdA.png">
<meta property="og:image" content="https://miro.medium.com/max/2073/1*Y117iNR_CnBtBh8MWVtUDg.png">
<meta property="og:image" content="https://miro.medium.com/max/3058/1*W34PwVsbTm_3EbJozaWWdA.jpeg">
<meta property="og:image" content="https://chiechie.github.io/2021/04/20/machine-learning/machine-learning_4-dl2_practice/img.png">
<meta property="article:published_time" content="2021-04-20T02:17:31.000Z">
<meta property="article:modified_time" content="2021-08-11T05:55:24.765Z">
<meta property="article:author" content="Chiechie">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="low level">
<meta property="article:tag" content="最佳实践">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kratzert.github.io/images/bn_backpass/BNcircuit.png">


<link rel="canonical" href="https://chiechie.github.io/2021/04/20/machine-learning/machine-learning_4-dl2_practice/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://chiechie.github.io/2021/04/20/machine-learning/machine-learning_4-dl2_practice/","path":"2021/04/20/machine-learning/machine-learning_4-dl2_practice/","title":"chapter 4.2 backpropagation和训练小技巧"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>chapter 4.2 backpropagation和训练小技巧 | Chiechie's Mini World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Chiechie's Mini World" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chiechie's Mini World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Set your course by the stars, not by the lights of passing ships. —— Omar Bradley</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">1.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#layer%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="nav-number">2.</span> <span class="nav-text">layer的输入输出</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#layer%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E5%88%9D%E5%A7%8B%E5%8C%96%E6%9D%83%E9%87%8D"><span class="nav-number">3.</span> <span class="nav-text">layer如何设置初始化权重？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E5%A4%AA%E9%95%BF"><span class="nav-number">4.</span> <span class="nav-text">训练过程太长？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chiechie</p>
  <div class="site-description" itemprop="description">a reader & thinker</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/04/20/machine-learning/machine-learning_4-dl2_practice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chapter 4.2 backpropagation和训练小技巧
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-04-20 10:17:31" itemprop="dateCreated datePublished" datetime="2021-04-20T10:17:31+08:00">2021-04-20</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-11 13:55:24" itemprop="dateModified" datetime="2021-08-11T13:55:24+08:00">2021-08-11</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="反向传播">反向传播</h2>
<blockquote>
<p>** The problem with Backpropagation is that it is a** <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Leaky_abstraction"><strong>leaky abstraction</strong></a><strong>.</strong>---Andrej Karpathy</p>
</blockquote>
<figure>
<img src="https://kratzert.github.io/images/bn_backpass/BNcircuit.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li><p>关于激活函数--sigmoid，另外一个容易忽视的事实是，其梯度在z=0.5时，达到最大--0.25，这意味着，每次梯度信号穿越过sigmoid门时，会衰减为1/4或者更多，如果使用基本 SGD，这会使网络的低层比高层的参数更新慢得多。<img src="https://miro.medium.com/max/1400/1*gkXI7LYwyGPLU5dn6Jb6Bg.png" alt="img" /></p></li>
<li><p>如果网络中使用 sigmoids 或者 tanh 作为激活函数，那么您应该警惕，初始化不会导致想训练过程完全饱和（ fully saturated）。</p></li>
<li><p>非线性函数ReLU：当输入小于0时，导数也为0，此时没有梯度信号能通过激活函数，这个现象叫“dead ReLU” 问题。如果初始权重没选好导致relu的输出为0，那么这个relu神经元再也不会被激活了，就永久保持死亡状态。在训练的过程中发现，大部分神经元（neuron，可能有40%）都是死亡状态。</p></li>
</ol>
<figure>
<img src="https://miro.medium.com/max/1400/1*g0yxlK8kEBw8uA1f82XQdA.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>注意：构建的神经网络中有 ReLUs单元时，应该始终对dead ReLUs保持警惕。在训练过程中，如果学习率设置的过于aggressive，常出现relu神经元死亡。</li>
<li>RNNs中的梯度爆炸（Exploding gradients in RNNs）,假设有一个简化的 RNN，不接受输入 x，只递归计算隐藏状态(等价地，输入 x 总是可以为零) ，RNN 是展开了T的时间步长。注意看反向通道时，会看到梯度信号逆着时间传播，并且是通过将隐藏状态乘以同一个矩阵循环矩阵<strong>Whh</strong>传播的，总是被同一个矩阵乘以(递归矩阵 Whh) ，中间穿插着非线性函数的反向传播。</li>
<li>当你用一个数 a 乘以另一个数 b (即 a * b * b * b * b * b * b * b)会发生什么。.)？如果 | b | &lt; 1，这个序列趋近零; 如果 | b | &gt; 1，这个序列趋近无穷。同样的事情也发生在 RNN 的反向传播过程中，不过 b 是一个矩阵，而不仅仅是一个数字，这个时候要算它的最大特征值。</li>
<li><strong>使用RNN时要注意</strong>: 警惕梯度截断（gradient clipping），或者使用LSTM.</li>
<li>额外的发现（Spotted in the Wild: DQN Clipping）：DQN中， 使用 <strong>target_q_t</strong>表示$ [reward * argmax_a Q(s’,a)]$，还有一个变量<strong>q_acted</strong>, which is <strong>Q(s,a)</strong> of the action that was taken. 二者相剑得到一个变量<strong>delta,</strong> 可以用使用l2损失最小化这个变量， <strong>tf.reduce_mean(tf.square()).</strong></li>
<li>损失函数是关于训练数据和网络权重的函数，其中训练数据是常数，权重是变量。因此，虽然损失关于训练数据的梯度很容易算，但是不去算，因为跟目标（更新权重）不一致。算损失关于权重的梯度，从而使用这个梯度去更新权重。</li>
<li>损失函数关于样本的梯度虽然对更新参数没有用，但是可以用来解释模型当前学习到了什么。</li>
<li>导数是什么？随着某个变量的变化，一个函数的变化量。表示函数对于当前变量值的敏感性。</li>
<li>考虑一个多层嵌套的函数f，对其应用链式法则，就可以到达终极变量的导数</li>
</ol>
<ul>
<li><p>f(x,y,z)=(x+y)z可以被分解为:q=x+y 和f=qz</p></li>
<li><p>分开来看，求偏导很简单，∂f/∂q=z, ∂f/∂z=q，∂q/∂x=1，∂q/∂y=1.</p></li>
<li><p>虽然，我们对中间变量的导数不感兴趣，使用链式法则可以沿着中间变量的导数得到f对于终极变量的导数，举个例子：</p>
<p>∂f/∂x=∂f/∂q.∂q/∂x</p>
<p>也就是两个中间变量相关的偏导数的乘积</p></li>
<li><p>激活函数sigmoid的导数比较简洁 <span class="math display">\[ \sigma(x)=\frac{1}{1+e^{-x}}  \rightarrow \frac{d \sigma(x)}{d x}=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}}=\left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)\left(\frac{1}{1+e^{-x}}\right)=(1-\sigma(x)) \sigma(x) \]</span></p></li>
<li><p><span class="math display">\[f(w, x)=\frac{1}{1+e^{-\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\right)}}\]</span>,</p></li>
</ul>
<blockquote>
<p>The derivative on each variable tells you the sensitivity of the whole expression on its value.*</p>
</blockquote>
<ul>
<li>导数是函数相对于某个自变量的敏感度；梯度是偏导数组成的向量。</li>
<li>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</li>
</ul>
<h2 id="layer的输入输出">layer的输入输出</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model_m = Sequential()</span><br><span class="line">model_m.add(Reshape((TIME_PERIODS, num_sensors),</span><br><span class="line">                    input_shape=(input_shape,)))</span><br><span class="line">model_m.add(Conv1D(<span class="number">100</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                   input_shape=(TIME_PERIODS, num_sensors)))</span><br><span class="line">model_m.add(Conv1D(<span class="number">100</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model_m.add(MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model_m.add(Conv1D(<span class="number">160</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model_m.add(Conv1D(<span class="number">160</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model_m.add(GlobalAveragePooling1D())</span><br><span class="line">model_m.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model_m.add(Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://miro.medium.com/max/2073/1*Y117iNR_CnBtBh8MWVtUDg.png" alt="Image for post" /><figcaption aria-hidden="true">Image for post</figcaption>
</figure>
<ul>
<li>Conv1D：　model_m.add(Conv1D(100, 10, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))：
<ul>
<li>输入：(timesteps, num_series)</li>
<li>输出：(timesteps - kerner_size+1, num_kernels)</li>
<li>参数个数：(kernel_size * num_sensors + 1) * num_kernels</li>
</ul></li>
<li>MaxPooling1D: model_m.add(MaxPooling1D(3))
<ul>
<li>输入：(timesteps, num_series)</li>
<li>输出：(timesteps//3, num_series)</li>
<li>备注：书的页数（channel）没变，但是每页的字数变成了1/3</li>
<li><img src="https://miro.medium.com/max/3058/1*W34PwVsbTm_3EbJozaWWdA.jpeg" title="fig:" alt="Image for post" /></li>
</ul></li>
<li>GlobalAveragePooling1D：model_m.add(GlobalAveragePooling1D())
<ul>
<li>输入：　(timesteps, num_series) 或者（feature_values, feature_detectors）</li>
<li>输出：　（１，num_series）或者（１, feature_detectors）</li>
<li>备注： 书的页数（channel）没变，但是每页的字数变成了1</li>
</ul></li>
<li>Dropout: model_m.add(Dropout(0.5))： 形状不变 输入：（１，num_series） 输出：（１，num_series）</li>
<li>LSTM：model.add(LSTM(units=128, dropout=0.5, return_sequences=True, input_shape=input_shape))
<ul>
<li>输入： (timestep, series)</li>
<li>输出：(timestep, units)</li>
</ul></li>
<li>Dense： model_m.add(Dense(num_classes, activation='softmax'))
<ul>
<li>输入：（１，num_series）</li>
<li>输出：（１，num_classes　×　２）</li>
</ul></li>
<li>BatchNormalization：
<ul>
<li>输入: (timestep, series)</li>
<li>输出：(timestep, series)</li>
<li>参数个数：４ * series (channel)
<ul>
<li><span class="math display">\[y=\gamma\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta \]</span></li>
</ul></li>
</ul></li>
</ul>
<h2 id="layer如何设置初始化权重">layer如何设置初始化权重？</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="./img.png" alt="3种初始化方法" /><figcaption aria-hidden="true">3种初始化方法</figcaption>
</figure>
<h2 id="训练过程太长">训练过程太长？</h2>
<p>保存训练结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 设置loss，优化算法</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=..., optimizer=...,</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">10</span></span><br><span class="line">checkpoint_filepath = <span class="string">&#x27;/tmp/checkpoint&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义回调函数</span></span><br><span class="line">model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(</span><br><span class="line">    filepath=checkpoint_filepath,</span><br><span class="line">    save_weights_only=<span class="literal">True</span>,</span><br><span class="line">    monitor=<span class="string">&#x27;val_accuracy&#x27;</span>,</span><br><span class="line">    mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">    save_best_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model weights are saved at the end of every epoch, if it&#x27;s the best seen</span></span><br><span class="line"><span class="comment"># so far.</span></span><br><span class="line"><span class="comment"># 3.在fit中传入回调函数，模型会一边训练一边存储</span></span><br><span class="line">model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 从缓存路径中加载模型</span></span><br><span class="line"><span class="comment"># The model weights (that are considered the best) are loaded into the model.</span></span><br><span class="line">model.load_weights(checkpoint_filepath)</span><br></pre></td></tr></table></figure>
<h2 id="参考">参考</h2>
<ol type="1">
<li><p>Hands-On Machine Learning with Scikit-Learn and TensorFlow, P334</p></li>
<li><p><a target="_blank" rel="noopener" href="https://keras.io/api/callbacks/model_checkpoint/">keras-model_checkpoint-官网文档</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://keras.io/api/layers/merging_layers/concatenate/">concatenate</a></p></li>
<li><p>http://cs231n.github.io/optimization-2/#intuitive</p></li>
<li><p>https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b</p></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/low-level/" rel="tag"># low level</a>
              <a href="/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/" rel="tag"># 最佳实践</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/19/reinforcement_learning/rl1_basic-concepts/" rel="prev" title="强化学习1 基本概念">
                  <i class="fa fa-chevron-left"></i> 强化学习1 基本概念
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/04/20/reinforcement_learning/rl2_markovprocess/" rel="next" title="强化学习2 马尔可夫过程">
                  强化学习2 马尔可夫过程 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiechie</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
