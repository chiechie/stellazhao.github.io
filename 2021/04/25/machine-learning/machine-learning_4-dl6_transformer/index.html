<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chiechie.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"æœç´¢...","empty":"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœï¼š${query}","hits_time":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœï¼ˆç”¨æ—¶ ${time} æ¯«ç§’ï¼‰","hits":"æ‰¾åˆ° ${hits} ä¸ªæœç´¢ç»“æœ"}}</script><script src="/js/config.js"></script>
<meta name="description" content="Transformeræ€»ç»“  Transformeræ˜¯google2016å¹´åœ¨ã€Šattention is all you needã€‹æå‡ºçš„ä¸€ä¸ªæœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå…¸å‹çš„seq2seqæ¶æ„ã€‚ Transformerçš„äº®ç‚¹åœ¨äºå°†attentionå’Œself-attentionå®Œå…¨å‰¥ç¦»å¼€ä¹‹å‰rnnçš„ç»“æ„ï¼Œåªè·Ÿdenseå±‚ç»„åˆã€‚ Transformerè·ŸRNNæ²¡æœ‰å…³ç³», åªæœ‰attentionå’Œå…¨è¿æ¥å±‚">
<meta property="og:type" content="article">
<meta property="og:title" content="chapter 4.6 æ·±åº¦å­¦ä¹ 6 Transformer">
<meta property="og:url" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/index.html">
<meta property="og:site_name" content="Chiechie&#39;s Mini World">
<meta property="og:description" content="Transformeræ€»ç»“  Transformeræ˜¯google2016å¹´åœ¨ã€Šattention is all you needã€‹æå‡ºçš„ä¸€ä¸ªæœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå…¸å‹çš„seq2seqæ¶æ„ã€‚ Transformerçš„äº®ç‚¹åœ¨äºå°†attentionå’Œself-attentionå®Œå…¨å‰¥ç¦»å¼€ä¹‹å‰rnnçš„ç»“æ„ï¼Œåªè·Ÿdenseå±‚ç»„åˆã€‚ Transformerè·ŸRNNæ²¡æœ‰å…³ç³», åªæœ‰attentionå’Œå…¨è¿æ¥å±‚">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_embeddings.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_encoder_with_tensors.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_positional_encoding_vectors.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_positional_encoding_example.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_6.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_7.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_attention_heads_qkv.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_attention_heads_z.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_2.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_3.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_self-attention-matrix-calculation.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_self-attention-matrix-calculation-2.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_attention_heads_weight_matrix_o.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_multi-headed_self-attention-recap.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_9.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_10.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_resideual_layer_norm.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_resideual_layer_norm_2.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_resideual_layer_norm_3.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_11.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_17.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_18.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_1.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_5.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_8.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_13.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_decoding_1.gif">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_decoding_2.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_12.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/Transformer_decoder.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_decoder_output_softmax.gif">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_14.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_15.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_encoders_decoders.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_encoder_decoder_stack.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/output_target_probability_distributions.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_output_trained_model_probability_distributions.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/img_20.png">
<meta property="article:published_time" content="2021-04-24T16:04:13.000Z">
<meta property="article:modified_time" content="2021-07-22T07:58:29.325Z">
<meta property="article:author" content="Chiechie">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="attention">
<meta property="article:tag" content="ç¥ç»ç½‘ç»œ">
<meta property="article:tag" content="æ¨¡å‹å¯è§†åŒ–">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="Bert">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/transformer_embeddings.png">


<link rel="canonical" href="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/","path":"2021/04/25/machine-learning/machine-learning_4-dl6_transformer/","title":"chapter 4.6 æ·±åº¦å­¦ä¹ 6 Transformer"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>chapter 4.6 æ·±åº¦å­¦ä¹ 6 Transformer | Chiechie's Mini World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Chiechie's Mini World" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="åˆ‡æ¢å¯¼èˆªæ " role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chiechie's Mini World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Set your course by the stars, not by the lights of passing ships. â€”â€” Omar Bradley</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          æ–‡ç« ç›®å½•
        </li>
        <li class="sidebar-nav-overview">
          ç«™ç‚¹æ¦‚è§ˆ
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#transformer%E6%80%BB%E7%BB%93"><span class="nav-number">1.</span> <span class="nav-text">Transformeræ€»ç»“</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">é™„å½•</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%92%8C%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="nav-number">2.1.</span> <span class="nav-text">è¾“å…¥å’Œä½ç½®ç¼–ç </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4self-attention"><span class="nav-number">2.2.</span> <span class="nav-text">å¤šå¤´self attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dense%E5%B1%82"><span class="nav-number">2.3.</span> <span class="nav-text">denseå±‚</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder-block-%E5%A4%9A%E5%A4%B4self-attention-dense-layer"><span class="nav-number">2.4.</span> <span class="nav-text">encoder block &#x3D; å¤šå¤´self-attention + dense layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E5%92%8Clayer-normalization"><span class="nav-number">2.5.</span> <span class="nav-text">æ®‹å·®è¿æ¥å’Œlayer-normalization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder-network-6-encoder-block"><span class="nav-number">2.6.</span> <span class="nav-text">encoder network &#x3D; 6 * encoder block</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4attention"><span class="nav-number">2.7.</span> <span class="nav-text">å¤šå¤´attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder-block-%E5%A4%9A%E5%A4%B4self-attention-%E5%A4%9A%E5%A4%B4attention-dense-layer"><span class="nav-number">2.8.</span> <span class="nav-text">decoder block &#x3D; å¤šå¤´self-attention + å¤šå¤´attention + dense layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder-network-6-decoder-block"><span class="nav-number">2.9.</span> <span class="nav-text">decoder network &#x3D; 6 * decoder block</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformer-encoder-network-decoder-network"><span class="nav-number">2.10.</span> <span class="nav-text">Transformer &#x3D; encoder network+ decoder network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">2.11.</span> <span class="nav-text">è®­ç»ƒ</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E7%9A%84%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90"><span class="nav-number">2.12.</span> <span class="nav-text">åº”ç”¨çš„ä¸€ä¸ªä¾‹å­</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">3.</span> <span class="nav-text">å‚è€ƒ</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chiechie</p>
  <div class="site-description" itemprop="description">a reader & thinker</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">æ—¥å¿—</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">åˆ†ç±»</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">æ ‡ç­¾</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="è¿”å›é¡¶éƒ¨">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chapter 4.6 æ·±åº¦å­¦ä¹ 6 Transformer
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">å‘è¡¨äº</span>

      <time title="åˆ›å»ºæ—¶é—´ï¼š2021-04-25 00:04:13" itemprop="dateCreated datePublished" datetime="2021-04-25T00:04:13+08:00">2021-04-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">æ›´æ–°äº</span>
        <time title="ä¿®æ”¹æ—¶é—´ï¼š2021-07-22 15:58:29" itemprop="dateModified" datetime="2021-07-22T15:58:29+08:00">2021-07-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">åˆ†ç±»äº</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">æœºå™¨å­¦ä¹ </span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="transformeræ€»ç»“">Transformeræ€»ç»“</h1>
<ul>
<li>Transformeræ˜¯google2016å¹´åœ¨ã€Šattention is all you needã€‹æå‡ºçš„ä¸€ä¸ªæœºå™¨ç¿»è¯‘æ¨¡å‹ï¼Œæ˜¯ä¸€ä¸ªå¾ˆå…¸å‹çš„seq2seqæ¶æ„ã€‚</li>
<li>Transformerçš„äº®ç‚¹åœ¨äºå°†attentionå’Œself-attentionå®Œå…¨å‰¥ç¦»å¼€ä¹‹å‰rnnçš„ç»“æ„ï¼Œåªè·Ÿdenseå±‚ç»„åˆã€‚</li>
<li>Transformerè·ŸRNNæ²¡æœ‰å…³ç³», åªæœ‰attentionå’Œå…¨è¿æ¥å±‚</li>
<li>Transformeræ¯”æ‰€æœ‰çš„rnn+attentionæ•ˆæœéƒ½è¦å¥½ï¼Œæœºå™¨ç¿»è¯‘çš„ç‹è€…</li>
<li>Transformeræ˜¯ä¸€ä¸ªç¿»è¯‘æ¨¡å‹ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œegå¾·è¯‘è‹±ï¼Œè¾“å…¥ä¸€ä¸ªå¾·æ–‡å¥å­ï¼Œè¾“å‡ºåä¸€å¥è‹±æ–‡ã€‚</li>
<li>Transformerç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼šencoderså’Œdecoders</li>
<li>encodersç”±6ä¸ªencoderå †å è€Œæˆï¼Œdecodersç”±6ä¸ªdecoderå †å è€Œæˆï¼Œä¸€ä¸ªencoderæˆ–ä¸€ä¸ªdecoderå«åšä¸€ä¸ªblockã€‚</li>
<li>encodersçš„æ¯ä¸ªblockæœ‰2å±‚ï¼šself-attentionå’Œdenseï¼Œæ¯ä¸ªblockç»“æ„ç›¸åŒï¼Œä½†ä¸å…±äº«æƒé‡ã€‚</li>
<li>decodersçš„æ¯ä¸ªblockæœ‰3å±‚ï¼šself-attentionï¼Œattentionå’Œdenseï¼Œå…¶ä¸­attentonç”¨æ¥å…³æ³¨encoderçš„è¾“å‡ºï¼Œ</li>
<li>attentionæŠ€æœ¯çš„æ¼”è¿›ï¼šattention + åŸºäºrnnçš„seq2seq --&gt; self-attention + lstm --&gt; attention/self attention + dense.</li>
<li>"å¤šå¤´"æ³¨æ„æœºåˆ¶æ‰©å±•äº†æ¨¡å‹å…³æ³¨ä¸åŒä½ç½®çš„èƒ½åŠ›, ç»™äºˆattentionå±‚å¤šä¸ª"è¡¨ç¤ºå­ç©ºé—´", æ¯ä¸ªå¤´æœ‰è‡ªå·±çš„ä¸€ç»„query / key / value æƒé‡çŸ©é˜µï¼Œå½“äºå®¡è§†å¤šæ¬¡ä¸Šä¸‹æ–‡ï¼Œ</li>
</ul>
<h1 id="é™„å½•">é™„å½•</h1>
<h2 id="è¾“å…¥å’Œä½ç½®ç¼–ç ">è¾“å…¥å’Œä½ç½®ç¼–ç </h2>
<ol type="1">
<li>å’Œä¸€èˆ¬çš„NLPä»»åŠ¡ä¸€æ ·ï¼Œç”¨embedding algorithmå°†æ¯ä¸ªè¾“å…¥å•è¯è½¬åŒ–ä¸ºè¯å‘é‡ä¹‹åï¼Œæ‰èƒ½ä½œä¸ºTransformerçš„è¾“å…¥ä½¿ç”¨ã€‚</li>
<li>å¦‚ä¸‹å›¾ï¼Œ3ä¸ªå•è¯è¢«embeddedä¸º3ä¸ª512ç»´çš„å‘é‡ <img src="./transformer_embeddings.png" /></li>
<li>æ¥ä¸‹æ¥ï¼Œå°†è¿™3ä¸ª512ç»´çš„è¯å‘é‡ä¼ å…¥self-attentionå’Œdenseå±‚ <img src="./transformer_encoder_with_tensors.png" /></li>
<li>ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰è¡¨ç¤ºåºåˆ—çš„é¡ºåº, ä¸ºäº†è®©æ¨¡å‹å­¦ä¹ åˆ°è¯çš„é¡ºåºå…³ç³»ï¼Œtransformerå‘æ¯ä¸ªè¾“å…¥embeddingå‘é‡ï¼ˆx1ï¼‰åˆåŠ ä¸Šä¸€ä¸ªä½ç½®ç¼–ç å‘é‡ï¼ˆt1ï¼‰ã€‚ <img src="./transformer_positional_encoding_vectors.png" alt="img" /></li>
<li>å‡è®¾embeddingçš„ç»´åº¦æ˜¯4ï¼Œé‚£ä¹ˆå®é™…çš„ä½ç½®ç¼–ç çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„: <img src="./transformer_positional_encoding_example.png" alt="img" /></li>
</ol>
<h2 id="å¤šå¤´self-attention">å¤šå¤´self attention</h2>
<ol type="1">
<li>æ¯ä¸ªå•å¤´self-attentionå±‚æœ‰ä¸‰ä¸ªå‚æ•°çŸ©é˜µï¼Œå•å¤´çš„context vectoræ˜¯ä¸€ä¸ªd * mç»´çš„çŸ©é˜µ <img src="./img_6.png" alt="å•å¤´self-attention" /></li>
<li>å¤šå¤´attentionå±‚å°±æœ‰3<em>lä¸ªå‚æ•°çŸ©é˜µï¼ˆlä»£è¡¨å¤´çš„ä¸ªæ•°ï¼‰ï¼Œå¤šå¤´çš„context vectoræ˜¯ä¸€ä¸ª(dl) </em>mç»´çš„çŸ©é˜µ <img src="./img_7.png" alt="å¤šå¤´self-attention" /> <img src="./transformer_attention_heads_qkv.png" alt="img" /></li>
<li>ç±»ä¼¼ä¸Šé¢æåˆ°çš„å•å¤´self-attentionè®¡ç®—ï¼Œæˆ‘ä»¬ç°åœ¨åªæ˜¯ç”¨8ä¸ªä¸åŒçš„æƒé‡çŸ©é˜µç®—äº†8æ¬¡ï¼Œå¹¶ä¸”å¾—åˆ°äº†8ä¸ªä¸åŒçš„ z çŸ©é˜µ <img src="./transformer_attention_heads_z.png" alt="img" /></li>
<li>å½“è§£è¯»ä¸€ä¸ªå¥å­ä¸­çš„ä¸€ä¸ªwordæ—¶ï¼Œ Transformeä¸­çš„encoderé€šè¿‡self-attentionæ¥å›é¡¾ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ‰¾å‡ºè¾“å…¥åºåˆ—ä¸­çš„é‡è¦çš„wordï¼Œä»è€Œæ›´å…³æ³¨é‡è¦ä¿¡æ¯ï¼Œå¹¶ä¸”ä¼šæŠŠé‡è¦wordçš„value vectorç¼–ç åˆ°è‡ªå·±çš„å‘é‡é‡Œé¢å»ã€‚ RNNé€šè¿‡hidden stateç­–ç•¥ï¼Œä½¿å¾—å®ƒå°†å½“å‰è¯ä¸ä¸Šä¸‹æ–‡ï¼ˆå‡†ç¡®æ¥è¯´åªæœ‰ä¸Šæ–‡ï¼‰çš„ä¿¡æ¯è¿›è¡Œèåˆã€‚</li>
<li>self-attentionçš„è¾“å…¥æ˜¯mä¸ªè¯å‘é‡ï¼Œè¾“å‡ºæ˜¯mä¸ªcontext vectorï¼Œvectorè¡¨ç¤ºèåˆäº†ä¸Šä¸‹æ–‡ä¹‹åï¼Œå¯¹wordå†æ¬¡ç¼–ç ã€‚ <img src="./img_2.png" alt="img_2.png" /> <img src="./img_3.png" alt="img_3.png" /></li>
<li>å¦‚ä½•å®ç°self-attention layerï¼Ÿå³å¦‚ä½•å®ç°è®¡ç®—context vectorï¼Ÿ</li>
</ol>
<ul>
<li>step1: å¯¹äºæ¯ä¸ªè¾“å…¥çš„wordçš„æ­¤å‘é‡åˆ†åˆ«è®¡ç®—3ä¸ªè¡¨ç¤ºå‘é‡ï¼š
<ul>
<li>Query vectorï¼š <span class="math inline">\(q_i\)</span>, to match othersï¼›</li>
<li>Key vectorï¼š<span class="math inline">\(k_i\)</span>, to be matchedï¼›</li>
<li>Value vectorï¼š<span class="math inline">\(v_i\)</span> to be weighted averaged ã€‚</li>
<li>è¿™äº›å‘é‡æ˜¯é€šè¿‡å°†è¾“å…¥çš„è¯å‘é‡åšä¸‰æ¬¡çº¿æ€§å˜æ¢å¾—åˆ°çš„ï¼Œå½“ç„¶å¯¹åº”çš„ä¸‰ä¸ªçŸ©é˜µæ˜¯éœ€è¦å­¦ä¹ çš„ã€‚</li>
<li>è¯·æ³¨æ„ï¼Œå¾—åˆ°çš„æ–°å‘é‡çš„ç»´åº¦ï¼ˆ64ï¼‰æ¯”embeddingï¼ˆ512ï¼‰å°ã€‚è¿™ä¹ˆè®¾è®¡ä¸ºäº†ä¿è¯ï¼ŒåŠ å…¥å¤šå¤´self-attentionä¹‹åï¼Œencoderçš„è¾“å‡ºå’Œè¾“å…¥ç»´åº¦ï¼ˆ512ï¼‰è¿˜èƒ½ä¿æŒä¸€è‡´ã€‚ &gt; 8å¤´ * 64 = 512</li>
</ul></li>
<li>step2: è®¡ç®—æƒé‡åˆ†æ•°, ç›®çš„æ˜¯é‡åŒ–å…¶ä»–å•è¯åº”è¯¥è¢«å…³æ³¨çš„ç¨‹åº¦ã€‚
<ul>
<li>è®¡ç®—å½“å‰å•è¯çš„query vectorå’Œå…¶ä»–å•è¯çš„key vectorçš„å†…ç§¯</li>
</ul></li>
<li>step3: æƒé‡åˆ†æ•°å½’ä¸€åŒ–ã€‚å°†ä¸Šä¸€æ­¥ç®—å‡ºæ¥çš„æƒé‡å¾—åˆ†å‘é‡ï¼ˆé•¿åº¦ä¸ºmï¼‰è¿›è¡Œå½’ä¸€åŒ–ï¼Œç›®çš„æ˜¯è®©æƒé‡ä¸è¦å—åˆ°key vectorçš„é•¿åº¦ï¼ˆpaperé‡Œé¢æ˜¯64ï¼‰çš„å½±å“ï¼Œè¿™æ ·ç®—æ¢¯åº¦å°±æ›´ç¨³å®šã€‚</li>
<li>step4ï¼šå°†å½’ä¸€åŒ–çš„æƒé‡å¾—åˆ†å‘é‡ï¼ˆé•¿åº¦ä¸ºmï¼‰é€å…¥softmaxï¼Œç›®çš„æ˜¯å¾—åˆ°ä¸€ä¸ªæ¦‚ç‡å‘é‡ï¼Œæ‰€æœ‰æ•°å€¼åŠ èµ·æ¥ä¸º1ã€‚ç¤ºæ¯ä¸ªå•è¯åº”è¯¥è¢«å½“å‰å•è¯å…³æ³¨çš„ç¨‹åº¦ï¼Œå€¼è¶Šå¤§è¶Šåº”è¯¥è¢«å…³æ³¨ã€‚ã€‚</li>
<li>step5ï¼šè®¡ç®—contect vectorã€‚å°†æ¯ä¸ªå•è¯çš„value vectorä½¿ç”¨æƒé‡æ¦‚ç‡å‘é‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œä½œä¸ºå½“å‰å•è¯çš„self-attentionè¡¨è¾¾ï¼ˆè¾“å‡ºé•¿åº¦ä¸º64 * 8ä¸ªå¤´ = 512ï¼‰ã€‚</li>
</ul>
<ol start="6" type="1">
<li>æŠŠåˆšåˆšçš„è¿‡ç¨‹å†åŠ å…¥ä¸€ä¸ªç»´åº¦ï¼Œä¹Ÿå°±æ˜¯ä»ä¸€ä¸ªå•è¯ å˜æˆ å¤šä¸ªå•è¯ï¼Œè®¡ç®—ä»–ä»¬å„è‡ªçš„self-attentionè¡¨ç¤ºï¼Œç”¨çŸ©é˜µè¡¨è¾¾è¯¥è®¡ç®—æµç¨‹ï¼š</li>
</ol>
<ul>
<li>ç¬¬ä¸€æ­¥æ˜¯è®¡ç®— Queryã€ Key å’Œ Value çŸ©é˜µï¼š
<ul>
<li>æˆ‘ä»¬å°†embeddingså¡åˆ°ä¸€ä¸ªçŸ©é˜µxï¼šè¡Œæ•°è¡¨ç¤ºå•è¯ä¸ªæ•°ï¼Œåˆ—è¡¨ç¤ºembeddingçš„é•¿åº¦</li>
<li>æƒé‡çŸ©é˜µ(WQã€ WKã€ WVï¼‰ï¼šè¡Œä»£è¡¨embeddingå‘é‡çš„é•¿åº¦ï¼Œåˆ—åˆ†åˆ«ä»£è¡¨queryç©ºé—´ï¼Œkeyç©ºé—´ï¼Œvalueç©ºé—´çš„ç»´åº¦</li>
<li>Xåˆ†åˆ«å’Œè¿™å‡ ä¸ªçŸ©é˜µç›¸ä¹˜ <img src="./transformer_self-attention-matrix-calculation.png" /></li>
</ul></li>
<li>ç¬¬äºŒæ­¥ï¼Œåˆ©ç”¨ç¬¬ä¸€æ­¥çš„ç»“æœæ¥è®¡ç®—attentionçš„è¾“å‡ºï¼Œç”¨ä¸€ä¸ªçŸ©é˜µè®¡ç®—æ¥è¡¨è¾¾ï¼Œç®€æ´ä¼˜é›… <img src="./transformer_self-attention-matrix-calculation-2.png" /></li>
</ul>
<h2 id="denseå±‚">denseå±‚</h2>
<ol type="1">
<li>åé¢æ€ä¹ˆè·Ÿdenseå±‚è¿›è¡Œè¡”æ¥å‘¢ï¼Ÿ1. å°†è¿™8ä¸ªzçŸ©é˜µè¿›è¡Œåˆ—æ‹¼æ¥ï¼ˆconcatï¼‰ï¼›2. æ‹¼æ¥åçš„çŸ©é˜µå¤§å°ä¸ºm<em>(8</em>64) = m *512ï¼Œä¸¢å…¥denseå±‚ <img src="./transformer_attention_heads_weight_matrix_o.png" alt="img" /></li>
<li>è¿™å°±æ˜¯multi-headed self-attentionçš„å¤§éƒ¨åˆ†å†…å®¹ã€‚æŠŠæ•´ä¸ªè¿‡ç¨‹æ”¾åœ¨ä¸€ä¸ªå›¾ä¸­æè¿°ï¼š <img src="./transformer_multi-headed_self-attention-recap.png" alt="img" /></li>
</ol>
<h2 id="encoder-block-å¤šå¤´self-attention-dense-layer">encoder block = å¤šå¤´self-attention + dense layer</h2>
<ol type="1">
<li>encoder block â‰ˆ å¤šå¤´self-attention + denseã€‚encoder blockçš„è¾“å…¥ç»´åº¦æ˜¯512Ã—ğ‘šï¼Œè¾“å‡ºç»´åº¦æ˜¯512Ã—ğ‘š.</li>
<li>ä»¥ä¸€ä¸ªçŸ­å¥ä¸ºä¾‹,çœ‹çœ‹encoderçš„å¤„ç†æµç¨‹:
<ol type="1">
<li>å°†[x1ï¼Œ...,xm]è¾“å…¥å¤šå¤´self-attentionå±‚, å¾—åˆ°mä¸ªcontext vector</li>
<li>å°†mä¸ªcontext vectorè¾“å…¥dense + reluï¼Œå¾—åˆ°mä¸ªé‡æ–°ç¼–ç åçš„å‘é‡[u1,...,um]ï¼Œdenseå±‚å¯¹mä¸ªwordç”Ÿæ•ˆã€‚</li>
<li>å°†[u1,...,um]ä¼ é€’ç»™ä¸‹ä¸€ä¸ªç¼–ç å™¨encoder2ä½œä¸ºè¾“å…¥ã€‚ <img src="./img_9.png" alt="img_9.png" /> <img src="./img_10.png" alt="img_10.png" /></li>
</ol></li>
</ol>
<h2 id="æ®‹å·®è¿æ¥å’Œlayer-normalization">æ®‹å·®è¿æ¥å’Œlayer-normalization</h2>
<ol type="1">
<li>æ¯ä¸ªencoder blockä¸­ï¼Œself-attentionå’Œdenseéƒ½æœ‰ä¸€ä¸ªæ®‹å·®è¿æ¥ï¼Œç„¶åæ¥ä¸€ä¸ªå±‚æ ‡å‡†åŒ–ï¼ˆlayer-normalizationï¼‰ã€‚ <img src="./transformer_resideual_layer_norm.png" alt="img" /></li>
<li>å¦‚æœæˆ‘ä»¬å°†æ®‹å·®è¿æ¥å’Œlayer normalizationç”¨çŸ©é˜µè¡¨ç¤ºï¼Œå°±æ˜¯ä¸‹å›¾ï¼š <img src="./transformer_resideual_layer_norm_2.png" alt="img" /></li>
<li>decoderçš„sub-layers ä¹ŸåŒæ ·ç”¨åˆ°äº†add &amp; normalizationçš„è®¾è®¡ã€‚ ä¸‹å›¾æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„Transformeræ¶æ„ï¼šç”±2ä¸ªstacked encoderå’Œ2ä¸ªstacked decoderç»„æˆã€‚ <img src="./transformer_resideual_layer_norm_3.png" alt="img" /></li>
<li>æ›´å‡†ç¡®æ¥è¯´ encoder block = positional codeing + å¤šå¤´self-attention + add &amp; normalizatioin + dense layer + add &amp; normalizatioin</li>
</ol>
<h2 id="encoder-network-6-encoder-block">encoder network = 6 * encoder block</h2>
<p>encoder network is a stack of 6 encoder blocks.</p>
<figure>
<img src="./img_11.png" alt="img_11.png" /><figcaption aria-hidden="true">img_11.png</figcaption>
</figure>
<h2 id="å¤šå¤´attention">å¤šå¤´attention</h2>
<ol type="1">
<li>å•å¤´attentionå±‚ï¼Œè¦è®¡ç®—3ä¸ªå‘é‡ï¼Œå…ˆè®¡ç®—qè·Ÿæ¯ä¸ªkçš„ç›¸å…³æ€§ï¼ˆæ±‚å†…ç§¯ï¼‰ï¼Œç„¶ååšsoftmaxå˜æ¢å¾—åˆ°æƒé‡å¾—åˆ†ï¼Œå¯¹æ‰€æœ‰çš„vä½¿ç”¨è¯¥æƒé‡æ±‚å’Œï¼Œå¾—åˆ°decoderçš„è¿™ä¸ªè¯ç›¸å¯¹encoderçš„è¡¨ç¤ºï¼Œä¹Ÿå°±æ˜¯context vectorã€‚ <img src="./img.png" alt="å•å¤´atttion" /></li>
<li>decoderä¸­çš„attentionï¼Œqæ¥è‡ªä¸¤éƒ¨åˆ†ï¼šä¸‹é¢çš„decoderçš„è¾“å‡ºå’Œæœ€ä¸Šé¢encoderä¸­çš„KeysçŸ©é˜µå’ŒValuesçŸ©é˜µ ã€‚ <img src="img_17.png" alt="img_17.png" /> <img src="img_18.png" alt="img_18.png" /></li>
<li>Attention å±‚çš„Queries</li>
</ol>
<p><img src="./img_1.png" alt="img_1.png" /> <img src="./img_5.png" alt="img_5.png" /><br />
<img src="./img_8.png" alt="img_8.png" /></p>
<h2 id="decoder-block-å¤šå¤´self-attention-å¤šå¤´attention-dense-layer">decoder block = å¤šå¤´self-attention + å¤šå¤´attention + dense layer</h2>
<p><img src="./img_13.png" alt="img_13.png" /> 1. 1ä¸ªdecoder block â‰ˆ å¤šå¤´self-attention + å¤šå¤´attention + denseã€‚decoder blockçš„è¾“å…¥æœ‰ä¸¤ä¸ªï¼Œä¸€ä¸ªæ˜¯encoderçš„è¾“å‡ºï¼ˆ512Ã—mï¼‰ï¼Œ ä¸€ä¸ªæ¥è‡ªdecoderå½“å‰çš„çš„è¾“å‡ºï¼ˆ512Ã—t ï¼‰. 2. encodingé˜¶æ®µï¼ˆphaseï¼‰å®Œæˆåï¼Œæˆ‘ä»¬å¼€å§‹decodingé˜¶æ®µï¼Œdecodingçš„æ¯ä¸ªstepæè¿°å¦‚ä¸‹ï¼š <img src="./transformer_decoding_1.gif" alt="img" /> <img src="./transformer_decoding_2.png" alt="img" /> 1. æ¯ä¸ªæ—¶åˆ»ï¼Œè¾“å…¥ç¬¬6ä¸ªencoderäº§ç”Ÿçš„mä¸ªkeys vectorså’Œvalue vectorsï¼Œè¾“å‡ºä¸€ä¸ªelementçš„æ¦‚ç‡åˆ†å¸ƒpdfã€‚ 2. ä»pdfä¸­æŠ½æ ·å‡ºwordï¼ˆæˆ–è€…ç›´æ¥é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„wordï¼‰ï¼Œä½œä¸ºdecoderçš„è¾“å…¥ã€‚ 3. é‡å¤step1ï¼Œç›´åˆ°äº§ç”Ÿç»“æŸç¬¦ã€‚ 3. decoderä¹Ÿä¼šç”¨åˆ°ä½ç½®ç¼–ç . 4. decoderä¸­çš„self attention layers:ä¸encoderçš„è¿ä½œæ–¹å¼ç•¥æœ‰ä¸åŒ,åœ¨decoderä¸­ï¼Œself-attention layeråªèƒ½æ¥è§¦åˆ°è¾“å‡ºåºåˆ—ä¸­çš„å‰åŠéƒ¨åˆ†ã€‚æ€ä¹ˆåšåˆ°å‘¢ï¼Ÿåœ¨softmaxæ“ä½œä¹‹å‰ï¼Œmaskingåºåˆ—ååŠéƒ¨åˆ†(è®¾ä¸º -inf)</p>
<h2 id="decoder-network-6-decoder-block">decoder network = 6 * decoder block</h2>
<ol type="1">
<li><p>Decoder network is a stack of 6 decoder blocks.</p>
<p><img src="./img_12.png" /> <img src="./Transformer_decoder.png" /></p></li>
<li><p>stack decodersè¾“å‡ºä¸€ä¸ªæ•°å€¼å‘é‡ã€‚ æˆ‘ä»¬æ€ä¹ˆæŠŠå®ƒå˜æˆä¸€ä¸ªè¯å‘¢ï¼Ÿ è¿™æ˜¯æœ€åçš„denseå±‚+softmaxåœ¨åšçš„äº‹æƒ…ã€‚å®ƒå°†stack of decodersçš„è¾“å‡ºæ˜ å°„ä¸ºä¸€ä¸ªæƒé‡å‘é‡ã€‚ï¼ˆé•¿åº¦å°±æ˜¯è¾“å‡ºè¯æ±‡è¡¨çš„å¤§å°ï¼‰</p></li>
<li><p>è¿™ä¸ªå›¾ä»åº•éƒ¨å¼€å§‹ï¼Œç”Ÿæˆä¸€ä¸ªvectorä½œä¸ºdecoder stackçš„è¾“å‡ºã€‚ ç„¶åå®ƒè¢«è½¬æ¢æˆä¸€ä¸ªè¾“å‡ºå•è¯ã€‚ <img src="./transformer_decoder_output_softmax.gif" alt="img" /></p></li>
</ol>
<h2 id="transformer-encoder-network-decoder-network">Transformer = encoder network+ decoder network</h2>
<p><img src="./img_14.png" alt="img_14.png" /> <img src="./img_15.png" alt="img_15.png" /> <img src="./transformer_encoders_decoders.png" alt="Transformer=encoders+decoders" /> <img src="./transformer_encoder_decoder_stack.png" /></p>
<h2 id="è®­ç»ƒ">è®­ç»ƒ</h2>
<p>å­¦ä¹ çš„ç›®æ ‡ï¼Ÿ å°†æ¨¡å‹è¾“å‡ºä¸ç›®æ ‡è¾“å‡ºè¿›è¡Œæ¯”è¾ƒï¼Œç„¶åä½¿ç”¨åå‘ä¼ æ’­æ–¹æ³•è°ƒæ•´æ¨¡å‹çš„æƒé‡ï¼Œä½¿æ¨¡å‹è¾“å‡ºæ›´æ¥è¿‘ç›®æ ‡è¾“å‡ºã€‚</p>
<p>å¦‚ä½•æ¯”è¾ƒä¸¤ç§æ¦‚ç‡åˆ†å¸ƒï¼Ÿ å¯æŸ¥çœ‹ <a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-09-Visual-Information/">cross-entropy</a>å’Œ <a target="_blank" rel="noopener" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">Kullback-Leibler</a>. ã€‚</p>
<p>ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒï¼š</p>
<figure>
<img src="./output_target_probability_distributions.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>æ¨¡å‹è¾“å‡ºï¼š</p>
<figure>
<img src="./transformer_output_trained_model_probability_distributions.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<h2 id="åº”ç”¨çš„ä¸€ä¸ªä¾‹å­">åº”ç”¨çš„ä¸€ä¸ªä¾‹å­</h2>
<figure>
<img src="img_20.png" alt="img_20.png" /><figcaption aria-hidden="true">img_20.png</figcaption>
</figure>
<h1 id="å‚è€ƒ">å‚è€ƒ</h1>
<ol type="1">
<li>Bahdanau, Cho, &amp; Bengio. Neural machine translation by jointly learning to align and translate. In ICLR, 2015.</li>
<li>Cheng, Dong, &amp; Lapata. Long Short-Term Memory-Networks for Machine Reading. In EMNLP, 2016.</li>
<li>Vaswani et al. Attention Is All You Need. In NIPS, 2017.</li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=aJRsr39F4dI">Transformeræ¨¡å‹(2/2): ä»Attentionå±‚åˆ°Transformerç½‘ç»œ</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/attention/" rel="tag"># attention</a>
              <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># ç¥ç»ç½‘ç»œ</a>
              <a href="/tags/%E6%A8%A1%E5%9E%8B%E5%8F%AF%E8%A7%86%E5%8C%96/" rel="tag"># æ¨¡å‹å¯è§†åŒ–</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
              <a href="/tags/Bert/" rel="tag"># Bert</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/" rel="prev" title="chapter 4.5 æ·±åº¦å­¦ä¹ 5 Word2vec">
                  <i class="fa fa-chevron-left"></i> chapter 4.5 æ·±åº¦å­¦ä¹ 5 Word2vec
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/04/25/reading_notes/investment/pork-and-beef/" rel="next" title="ç‰›è‚‰æ˜¯ä¸æ˜¯æœªæ¥çš„è¶‹åŠ¿ä¹‹ä¸€ï¼Ÿ">
                  ç‰›è‚‰æ˜¯ä¸æ˜¯æœªæ¥çš„è¶‹åŠ¿ä¹‹ä¸€ï¼Ÿ <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiechie</span>
</div>
  <div class="powered-by">ç”± <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> å¼ºåŠ›é©±åŠ¨
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
