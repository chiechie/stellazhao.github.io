<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chiechie.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="词嵌入（word embedding）是指把一个词映射为一个实数域的向量，这个向量也叫词向量。 word2vec是一个词嵌入模型，有两个变形，跳字模型和连续词袋模型。跳字模型是基于中心词来生成背景词。连续词袋模型假设基于背景词来生成中心词。 word2vec模型学习到了一个词的两个表示（背景词向量和中心词向量），下游节点用的是哪个呢？  总结  怎么对一个word编码？最简单的就是one-hot">
<meta property="og:type" content="article">
<meta property="og:title" content="chapter 4.5 深度学习5 Word2vec">
<meta property="og:url" content="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/index.html">
<meta property="og:site_name" content="Chiechie&#39;s Mini World">
<meta property="og:description" content="词嵌入（word embedding）是指把一个词映射为一个实数域的向量，这个向量也叫词向量。 word2vec是一个词嵌入模型，有两个变形，跳字模型和连续词袋模型。跳字模型是基于中心词来生成背景词。连续词袋模型假设基于背景词来生成中心词。 word2vec模型学习到了一个词的两个表示（背景词向量和中心词向量），下游节点用的是哪个呢？  总结  怎么对一个word编码？最简单的就是one-hot">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/img2.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/img.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/image-20210730161649181.png">
<meta property="og:image" content="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/img1.png">
<meta property="og:image" content="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FjpuHAmzTik.png?alt=media&amp;token=47e0004c-bc7c-4fc5-af22-d227532a7548">
<meta property="og:image" content="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FsFuwRdSRxR.png?alt=media&amp;token=03f6ea4c-aee0-4e11-993f-468505022f8d">
<meta property="og:image" content="https://pic1.zhimg.com/v2-86184fb83baaf83c6acb1785ad00ada8_b.png">
<meta property="og:image" content="https://pic2.zhimg.com/v2-694995f782ec70a94315715edbaae5f9_b.png">
<meta property="og:image" content="https://pic2.zhimg.com/v2-71e036edb275e380e362a4c5f0f1feb5_b.png">
<meta property="article:published_time" content="2021-04-24T14:23:20.000Z">
<meta property="article:modified_time" content="2021-08-01T14:59:49.703Z">
<meta property="article:author" content="Chiechie">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="word2vec">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/img2.png">


<link rel="canonical" href="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/","path":"2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/","title":"chapter 4.5 深度学习5 Word2vec"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>chapter 4.5 深度学习5 Word2vec | Chiechie's Mini World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Chiechie's Mini World" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chiechie's Mini World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Set your course by the stars, not by the lights of passing ships. —— Omar Bradley</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.</span> <span class="nav-text">总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.1.</span> <span class="nav-text">连续跳字模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="nav-number">2.2.</span> <span class="nav-text">跳字模型架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E5%AF%BC"><span class="nav-number">2.3.</span> <span class="nav-text">跳字模型的推导</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7"><span class="nav-number">2.4.</span> <span class="nav-text">负采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8tensorflow%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.5.</span> <span class="nav-text">使用tensorflow实现一个跳字模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BB%BA%E6%A8%A1"><span class="nav-number">2.5.1.</span> <span class="nav-text">建模</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AF%84%E4%BC%B0"><span class="nav-number">2.5.2.</span> <span class="nav-text">评估</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%82%B9%E7%A7%AFor%E4%BD%99%E5%BC%A6"><span class="nav-number">2.6.</span> <span class="nav-text">点积or余弦</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec%E6%98%AF%E6%97%A0%E7%9B%91%E7%9D%A3%E8%BF%98%E6%98%AF%E6%9C%89%E7%9B%91%E7%9D%A3"><span class="nav-number">2.7.</span> <span class="nav-text">word2vec是无监督还是有监督？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">3.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chiechie</p>
  <div class="site-description" itemprop="description">a reader & thinker</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/04/24/machine-learning/machine-learning_4-dl5_word2vec/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chapter 4.5 深度学习5 Word2vec
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-04-24 22:23:20" itemprop="dateCreated datePublished" datetime="2021-04-24T22:23:20+08:00">2021-04-24</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-01 22:59:49" itemprop="dateModified" datetime="2021-08-01T22:59:49+08:00">2021-08-01</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <blockquote>
<p>词嵌入（word embedding）是指把一个词映射为一个实数域的向量，这个向量也叫词向量。</p>
<p>word2vec是一个词嵌入模型，有两个变形，跳字模型和连续词袋模型。跳字模型是基于中心词来生成背景词。连续词袋模型假设基于背景词来生成中心词。</p>
<p>word2vec模型学习到了一个词的两个表示（背景词向量和中心词向量），下游节点用的是哪个呢？</p>
</blockquote>
<h2 id="总结">总结</h2>
<ol type="1">
<li><p>怎么对一个word编码？最简单的就是one-hot，即，先构建一次词典，然后用该word在词典中的索引来表示。</p></li>
<li><p>但是，one-hot的方法无法表达词语间语义上的相似性，所以提出来了词向量的方法，其中使用最多的一种是word2vec方法。为什么说one-hot vector说表达不了word在语义上的相似性？举个例子，假如用余弦函数来描述word之间的相似度，那么one-hot的表达方式，就会使所有word之间的相似度为0，这样就丢失了词语的语义上的信息。</p></li>
<li><p>Wod2Vec跟传统的线性降维方法如PCA一样，本质上都是在寻找对原始输入（词语）的一种更经济、高效的表达。PCA和Word2vec的区别在于，两者对于何为「最优表达」的定义不一样：</p>
<ul>
<li>PCA认为重构误差最小的表达，就是最好的表达。即经过编码-解码之后,重构回原来空间时，信息丢失尽可能少。</li>
<li>Word2vec认为如果一个中心词的表达跟上下文表达关联最大，就是最好的表达。</li>
</ul></li>
<li><p>在Word2vec中，每个词语都有两个表示，分别为<strong>中心词向量</strong>和<strong>背景词向量</strong>，取决于此刻该词语的角色。为了学习每个单词的中心词向量和背景词向量，Word2vec提出了两种模型，分别是以中心词预测背景词的跳字模型（Skip-gram）模型，和以背景词去预测中心词的连续词袋（Continuous Bag-of-Words，CBOW）模型。</p></li>
<li><p>跳字模型（skip gram）--基于中心词来生成背景词。跳字模型就是将一个中心词映射成多个背景词，大概思路，</p>
<ol type="1">
<li><p>对词典中每个词，构建2个向量表示，这个是待估参数。</p></li>
<li><p>类似多分类问题，构造一个映射函数，输入中心词的one-hot（长度为<span class="math inline">\(\mathcal{V}\)</span>），输出背景词的概率分布（长度为<span class="math inline">\(\mathcal{V}\)</span>）</p></li>
<li><p>类似逻辑回归的参数估计，使用交叉熵作为损失函数。</p></li>
<li><p>总体来说，从low-level，跳字模型，就很像是一个多分类的逻辑回归，跳字模型保留的是参数，而逻辑回归需要用的是输出的概率。</p></li>
</ol></li>
<li><p>连续词袋模型（continous bag of words）是用背景词预测中心词的模型。以下图为例，一般使用连续词袋模型的背景词向量作为词的表征向量。</p></li>
</ol>
<h2 id="附录">附录</h2>
<h3 id="连续跳字模型">连续跳字模型</h3>
<p><img src="img2.png" /></p>
<figure>
<img src="./img.png" alt="CBOW" /><figcaption aria-hidden="true">CBOW</figcaption>
</figure>
<p>假设有这么一段话，“the”“man”“loves”“his”“son”，那么输入背景词序列--the”“man”“his”“son”，连续词带模型会输出中心词为loves的概率。</p>
<h3 id="跳字模型架构">跳字模型架构</h3>
<figure>
<img src="./image-20210730161649181.png" alt="image-20210730161649181" /><figcaption aria-hidden="true">image-20210730161649181</figcaption>
</figure>
<figure>
<img src="./img1.png" alt="跳字模型希望预测给定中心词时，背景词的条件概率" /><figcaption aria-hidden="true">跳字模型希望预测给定中心词时，背景词的条件概率</figcaption>
</figure>
<h3 id="跳字模型的推导">跳字模型的推导</h3>
<ul>
<li><p>词典索引集： <span class="math inline">\(\mathcal{V} =\{0,1, \ldots,|\mathcal{V}|-1\}\)</span></p></li>
<li><p>假设某个词在词典中的索引为i，这个词被表示成2个d维向量:</p>
<ul>
<li>中心词向量：<span class="math inline">\(\boldsymbol{v}_{i} \in \mathbb{R}^{d}\)</span></li>
<li>背景词向量：<span class="math inline">\(\boldsymbol{u}_{i} \in \mathbb{R}^{d}\)</span></li>
</ul></li>
<li><p>模型的输出是条件概率，假设中心词<span class="math inline">\(w_c\)</span>在词典中的索引为c，背景词<span class="math inline">\(w_o\)</span>在词典中的索引为o，那么给定中心词生成背景词的条件概率为</p>
<p><span class="math display">\[P\left(w_{o} \mid w_{c}\right)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum\limits_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)}\]</span></p></li>
<li><p>假设给定一个长度为 T 的文本序列，设时间步 t 的词为 w(t) 。假设给定中心词的情况下背景词的生成相互独立，当背景窗口大小为 m 时，跳字模型的似然函数即给定任一中心词生成所有背景词的概率 <span class="math display">\[\prod\limits_{t=1}^{T} P\left(w^{(t-j)} ,... w^{(t+j)}\mid w^{(t)}\right) = \prod\limits_{t=1}^{T} \prod\limits_{-m \leq j \leq m, j \neq 0} P\left(w^{(t+j)} \mid w^{(t)}\right)\]</span> 这里小于1或大于 T 的时间步可以被忽略。</p></li>
<li><p>参数估计方法：MLE, 即最大化如下对数似然函数来估计参数：</p>
<p><span class="math display">\[\max\limits_{u,v}  \prod\limits_{t=1}^{T} \prod\limits_{-m \leq j \leq m, j \neq 0} P\left(w^{(t+j)} \mid w^{(t)}\right)\]</span>​ 等价于最小化交叉熵: <span class="math inline">\(\min\limits_{u,v} -\sum\limits_{i=1}^{T} \sum\limits_{-m \leq j \leq m, j \neq 0} \log P\left(w^{(t+j)} \mid w^{(t)}\right)\)</span>​</p>
<p>由于 <span class="math display">\[\log P\left(w_{o} \mid w_{c}\right)= \log(\frac{\exp \left(\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}\right)}{\sum\limits_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)})=\boldsymbol{u}_{o}^{\top} \boldsymbol{v}_{c}-\log \left(\sum\limits_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)\right)\]</span>​</p>
<p>损失函数相对于<span class="math inline">\(v_c\)</span>​的梯度 <span class="math display">\[\begin{aligned} \frac{\partial \log P\left(w_{o} \mid w_{c}\right)}{\partial \boldsymbol{v}_{c}} &amp;=\boldsymbol{u}_{o}-\frac{\sum_{j \in \mathcal{V}} \exp \left(\boldsymbol{u}_{j}^{\top} \boldsymbol{v}_{c}\right) \boldsymbol{u}_{j}}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)} \\ &amp;=\boldsymbol{u}_{o}-\sum_{j \in \mathcal{V}}\left(\frac{\exp \left(\boldsymbol{u}_{j}^{\top} \boldsymbol{v}_{c}\right)}{\sum_{i \in \mathcal{V}} \exp \left(\boldsymbol{u}_{i}^{\top} \boldsymbol{v}_{c}\right)}\right) \boldsymbol{u}_{j} \\ &amp;=\boldsymbol{u}_{o}-\sum_{j \in \mathcal{V}} P\left(w_{j} \mid w_{c}\right) \boldsymbol{u}_{j} \end{aligned}\]</span>​</p></li>
</ul>
<p>训练结束后，对于词典中的任一索引为i的词，我们均得到该词作为中心词和背景词的两组词向量 <span class="math inline">\(v_i\)</span> 和 <span class="math inline">\(u_i\)</span> 。在自然语言处理应用中，一般使用跳字模型的中心词向量作为词的表征向量。</p>
<h3 id="负采样">负采样</h3>
<p>梯度上式的复杂度为|V|, 为了提高计算效率，有一个近似训练方案：负采样 和分层的softmax</p>
<p>负采样的想法是，模型的最后一层不用softmax，而是sigmoid,损失函数交叉熵：观测到就是正样本，没观测到就是负样本（通过对词典采样K次得到） <span class="math inline">\(P\left(w^{(t+j)} \mid w^{(t)}\right)=P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right) \prod\limits_{k=1, w_{k} \sim P(w)}^{K} P\left(D=0 \mid w^{(t)}, w_{k}\right)\)</span></p>
<p>取对数就是</p>
<p><span class="math inline">\(\begin{aligned}-\log P\left(w^{(t+j)} \mid w^{(t)}\right) &amp;=-\log P\left(D=1 \mid w^{(t)}, w^{(t+j)}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log P\left(D=0 \mid w^{(t)}, w_{k}\right) \\ &amp;=-\log \sigma\left(\boldsymbol{u}_{i_{t+j}}^{\top} \boldsymbol{y}_{i_{t}}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log \left(1-\sigma\left(\boldsymbol{u}_{h_{k}}^{\top} \boldsymbol{v}_{i_{t}}\right)\right) \\ &amp;=-\log \sigma\left(\boldsymbol{u}_{i_{t+j}}^{\top} \boldsymbol{y}_{i_{t}}\right)-\sum_{k=1, w_{k} \sim P(w)}^{K} \log \sigma\left(-\boldsymbol{u}_{h_{k}}^{\top} \boldsymbol{v}_{i_{t}}\right) \end{aligned}\)</span></p>
<h3 id="使用tensorflow实现一个跳字模型">使用tensorflow实现一个跳字模型</h3>
<h4 id="建模">建模</h4>
<p>word2vec将每个词表示成一个定长的向量，并使得这些向量能较好地表达不同词之间的相似和类比关系</p>
<ul>
<li>第一步：获取训练样本
<ul>
<li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FjpuHAmzTik.png?alt=media&amp;token=47e0004c-bc7c-4fc5-af22-d227532a7548" /></li>
</ul></li>
<li>第二步：构建1个神经网络
<ul>
<li>输入：上面的中心词的one-hot向量，形状为（词典大小，）</li>
<li>输出：中心词周围出现的词的概率向量（相近词的概率），形状也是（词典大小）</li>
<li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FsFuwRdSRxR.png?alt=media&amp;token=03f6ea4c-aee0-4e11-993f-468505022f8d" /></li>
<li>模型参数（or权重向量）的形状和含义：
<ul>
<li>网络 第一层有300（词向量长度）个单元，权重向量的形状是 (词典大小，词向量长度)，表示 每个词的中心词向量</li>
<li>网络 第二层有10000（词典大小）个单元，权重向量的形状是（词向量长度，词典大小），表示 每个词的背景词向量</li>
</ul></li>
</ul></li>
</ul>
<h4 id="评估">评估</h4>
<ul>
<li>影响词向量质量的三个因素：
<ul>
<li>训练数据的数量和质量</li>
<li>词向量的大小</li>
<li>训练算法</li>
</ul></li>
</ul>
<p>google提供了 测试数据 和 测试脚本</p>
<ul>
<li>先人工定义一些 近义词组，反义词组，不相关词组，计算这些词组的 余弦距离，看是否跟之前定义的语义距离 一致。</li>
<li>提供2份 相关性测试集（relation test set）:
<ul>
<li>word relation test set :<strong>./demo-word-accuracy.sh</strong>,</li>
<li>phrase relation test set:<strong>./demo-phrase-accuracy.sh</strong></li>
</ul></li>
<li>最好的结果：准确率 70% + 覆盖率 100%.</li>
</ul>
<h3 id="点积or余弦">点积or余弦</h3>
<ol type="1">
<li><p>点积和余弦, 差了一个量岗，即向量模长。Word2Vec训练的时候用的是点积，推断用的cos。</p></li>
<li><p>训练的时候之所以不用cos，而用点积，是因为如果先做cos，会将神经网络的输出限制到[-1, 1]，限制网络的表达能力</p></li>
<li><p>推断的时候，要用cos，例如找近义词。</p></li>
</ol>
<h3 id="word2vec是无监督还是有监督">word2vec是无监督还是有监督？</h3>
<figure>
<img src="https://pic1.zhimg.com/v2-86184fb83baaf83c6acb1785ad00ada8_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>现在大家一般说有监督/无监督，总结下来，本质是从两个维度描述：</p>
<p>维度1： 是否需要人工标注</p>
<p>维度2： 是否使用supervised data来更新参数，</p>
<p>两个维度的取值为「 是」，就是有监督</p>
<p>两个维度的取值为「 否」，就是无监督</p>
<p>对于两个流派来说，这个是能达成共识的。</p>
<p>而如果维度2 认为「是」，维度1认为「否」的情况呢？ （也就是word2vec的情况）</p>
<figure>
<img src="https://pic2.zhimg.com/v2-694995f782ec70a94315715edbaae5f9_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>做异常检测的时候，如果没有异常标记数据，</p>
<p>使用预测 + 极值检测 的方法，算是有监督还是无监督？</p>
<p>把预测 + 极值检测 看成一个黑盒，是无监督，</p>
<p>单独看时序预测，是自监督。</p>
<figure>
<img src="https://pic2.zhimg.com/v2-71e036edb275e380e362a4c5f0f1feb5_b.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<p>无监督训练中：一个模型在未标记的数据（或者全部数据） 上训练，使用了一个无监督的学习技术，然后对最后的带标记数据使用有监督学习的方式，进行微调；</p>
<p>无监督每次只训练一层，也可以直接训练整个模型</p>
<h2 id="参考">参考</h2>
<ol type="1">
<li>https://mk.woa.com/q/267975?strict=true&amp;ADTAG=daily</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1310.4546.pdf">word2vec论文--Distributed Representations of Words and Phrases and their Compositionality</a></li>
<li><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing/word2vec.html#%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B">dive into deep learning</a></li>
<li>https://www.quora.com/Is-Word2vec-a-supervised-unsupervised-learning-algorithm</li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/108625273">Sherlock：Self-Supervised Learning 入门介绍</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/chiechie/EasyML/tree/master/nlp">word2vecd代码</a></li>
<li>https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec-pretraining.html</li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/word2vec/" rel="tag"># word2vec</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/24/reinforcement_learning/rl6_advanced-value-based/" rel="prev" title="强化学习6 价值学习高级技巧">
                  <i class="fa fa-chevron-left"></i> 强化学习6 价值学习高级技巧
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/04/25/machine-learning/machine-learning_4-dl6_transformer/" rel="next" title="chapter 4.6 深度学习6 Transformer">
                  chapter 4.6 深度学习6 Transformer <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiechie</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
