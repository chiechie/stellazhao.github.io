<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chiechie.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="总结 决策树  决策树是一个预测算法，是使用贪婪的递归的方法来找到最优的预测结构。 构建一个决策树需要确定分支条件以及每个分支的预测值。构建分支条件即把特征空间划分为多个distinct and non-overlapping regions,\(R_1,\dots,R_J\)；第二步，对每个region定义一个response variable，作为该region的值。 构建好了一个决策树，想要">
<meta property="og:type" content="article">
<meta property="og:title" content="chapter 2.1 树模型-决策树介绍">
<meta property="og:url" content="https://chiechie.github.io/2021/04/16/machine-learning/machine-learning_2-trees_1/index.html">
<meta property="og:site_name" content="Chiechie&#39;s Mini World">
<meta property="og:description" content="总结 决策树  决策树是一个预测算法，是使用贪婪的递归的方法来找到最优的预测结构。 构建一个决策树需要确定分支条件以及每个分支的预测值。构建分支条件即把特征空间划分为多个distinct and non-overlapping regions,\(R_1,\dots,R_J\)；第二步，对每个region定义一个response variable，作为该region的值。 构建好了一个决策树，想要">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chiechie.github.io/2021/04/16/machine-learning/machine-learning_2-trees_1/img.png">
<meta property="article:published_time" content="2021-04-16T12:56:02.000Z">
<meta property="article:modified_time" content="2021-07-22T07:55:48.667Z">
<meta property="article:author" content="Chiechie">
<meta property="article:tag" content="人工智能">
<meta property="article:tag" content="树模型">
<meta property="article:tag" content="决策树">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chiechie.github.io/2021/04/16/machine-learning/machine-learning_2-trees_1/img.png">


<link rel="canonical" href="https://chiechie.github.io/2021/04/16/machine-learning/machine-learning_2-trees_1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://chiechie.github.io/2021/04/16/machine-learning/machine-learning_2-trees_1/","path":"2021/04/16/machine-learning/machine-learning_2-trees_1/","title":"chapter 2.1 树模型-决策树介绍"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>chapter 2.1 树模型-决策树介绍 | Chiechie's Mini World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Chiechie's Mini World" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chiechie's Mini World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Set your course by the stars, not by the lights of passing ships. —— Omar Bradley</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.</span> <span class="nav-text">总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="nav-number">1.1.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-cart"><span class="nav-number">1.2.</span> <span class="nav-text">决策树算法-CART</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">2.</span> <span class="nav-text">附录</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">2.1.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-id3"><span class="nav-number">2.2.</span> <span class="nav-text">决策树算法-ID3</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-c4.5"><span class="nav-number">2.3.</span> <span class="nav-text">决策树算法-C4.5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95-cart-1"><span class="nav-number">2.4.</span> <span class="nav-text">决策树算法-CART</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#regression-tree"><span class="nav-number">2.4.1.</span> <span class="nav-text">regression tree</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#classification-tree"><span class="nav-number">2.4.2.</span> <span class="nav-text">classification tree</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">3.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chiechie</p>
  <div class="site-description" itemprop="description">a reader & thinker</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/04/16/machine-learning/machine-learning_2-trees_1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          chapter 2.1 树模型-决策树介绍
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-04-16 20:56:02" itemprop="dateCreated datePublished" datetime="2021-04-16T20:56:02+08:00">2021-04-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-07-22 15:55:48" itemprop="dateModified" datetime="2021-07-22T15:55:48+08:00">2021-07-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="总结">总结</h1>
<h2 id="决策树">决策树</h2>
<ol type="1">
<li>决策树是一个预测算法，是使用贪婪的递归的方法来找到最优的预测结构。</li>
<li>构建一个决策树需要确定分支条件以及每个分支的预测值。构建分支条件即把特征空间划分为多个distinct and non-overlapping regions,<span class="math inline">\(R_1,\dots,R_J\)</span>；第二步，对每个region定义一个response variable，作为该region的值。</li>
<li>构建好了一个决策树，想要使用决策树做预测，步骤也分为两步：第一步是按照分支条件将新样本路由到指定的叶子结点，第二步将叶子结点对应的responsible varible作为该样本的预测值。</li>
<li>怎么得到分支条件呢？有三类构建决策树的算法：ID3，C4.5和cart。前两者可以构造多叉树，cart只能构造二叉树。 因为cart效果最好，现在通常就用它（例如sklean）。</li>
<li>ID3，C4.5和cart三类算法的大致思路一样，分为两步：第一步是将feature space切成多个boxes。为什么不是切成多个球？因为球没法填充整个feature space.</li>
<li>如何找到最优的切割boxes的方式，如果去遍历每一组partition of feature space，计算量太大了，通常采用greedy的方法。</li>
<li>构建决策树的过程中需要确定的参数：分支的个数，条件分支的条件，终止条件，叶子结点的值。</li>
</ol>
<h2 id="决策树算法-cart">决策树算法-CART</h2>
<ol type="1">
<li>cart的特色是构建的一个binary tree，每次分支条件都是将一个空间以分为2，变成2个子空间，每个叶子结点的值，即response variable都是一个常数，是这么的到的：</li>
</ol>
<ul>
<li>如果target var是一个连续变量，求落入该region的训练集的response均值，即<span class="math inline">\({y_n}\)</span>的均值，其实这个均值对应的是最小化suqared error。 构建一个决策树，主要是要确定partition，或者说分支条件，以及每个落入每个partition（或者说）中对应的预测值。</li>
<li>如果target var是一个离散变量，求众数对应的那个类别。</li>
</ul>
<ol start="2" type="1">
<li>怎么确定分支条件？找一个decision stump，使用纯度（purify）来衡量分支的质量，如果左边的data set 和右边的dataset 纯度 都很高，（其中的大部分样本的label很接近），就说切分的很好。对应到计算上面，就是找一个让平均不纯度最小的切分方式（decision stump）。</li>
<li>如何确定分支/切割的不纯度？
<ul>
<li>如果target var是一个连续变量，使用squared loss来描述impurity，（跟样本子集的均值比），</li>
<li>如果target var是一个离散变量，使用不一致样本比例来描述impurity，（跟样本子集的众数币），多分类的时候，不纯度常用giniindex</li>
<li>如果是多分类，经常使用Gini index来刻画不纯度。</li>
</ul></li>
<li>什么时候会停下来？当满足下面的条件时，也叫fully grown tree：
<ul>
<li>落入某个分支的样本的target都一样，不纯度取到最小了，</li>
<li>落入某个分支的样本的x都一样，没有decision stupms了。</li>
</ul></li>
<li>为何要剪枝(pruning)? a very bushy tree has got high variances,ie, over-fitting the data</li>
</ol>
<h1 id="附录">附录</h1>
<h2 id="基本概念">基本概念</h2>
<ul>
<li>信息增益: 衡量切分前后，样本纯度的提升or混乱度的下降。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IG = information before splitting (parent) — information after splitting (children)</span><br></pre></td></tr></table></figure>
<ul>
<li>具体的，有两个衡量纯度/混乱度的指标：Entropy 和 Gini Impurity
<ul>
<li>基尼系数（<strong>gini index</strong>）: <span class="math display">\[I_{G}=1-\sum_{j=1}^{c} p_{j}^{2}\]</span>
<ul>
<li><span class="math inline">\(p_j\)</span>: 落入该节点的样本中，第j类样本的占比</li>
<li>如果所有样本都属于某一类c，gini系数最小，为0。</li>
</ul></li>
<li>熵（entropy）：<span class="math display">\[I_{H}=-\sum_{j=1}^{c} p_{j} \log _{2}\left(p_{j}\right)\]</span>
<ul>
<li><span class="math inline">\(p_j\)</span>: 落入该节点的样本中，第j类样本的占比</li>
<li>如果所有样本都属于某一类c，熵最小，为0。</li>
</ul></li>
</ul></li>
</ul>
<h2 id="决策树算法-id3">决策树算法-ID3</h2>
<p>ID3, was the first of three Decision Tree implementations developed by Ross Quinlan</p>
<p>It builds a decision tree for the given data in a top-down fashion. each node of the tree, one feature is tested based on 最大熵降, and the results are used to split the sample set. This process is recursively done until the set in a given sub-tree is homogeneous (i.e. it contains samples belonging to the same category). The ID3 algorithm uses a greedy search.</p>
<p>Disadvantages:</p>
<ul>
<li>Data may be over-fitted or over-classified, if a small sample is tested.</li>
<li>Only one attribute at a time is tested for making a decision.</li>
<li>Does not handle numeric attributes and missing values.</li>
</ul>
<h2 id="决策树算法-c4.5">决策树算法-C4.5</h2>
<p>Improved version on ID 3 . The new features (versus ID3) are:</p>
<ul>
<li><ol type="i">
<li>accepts both continuous and discrete features;</li>
</ol></li>
<li><ol start="2" type="i">
<li>handles incomplete data points;</li>
</ol></li>
<li><ol start="3" type="i">
<li>solves over-fitting problem by (very clever) bottom-up technique usually known as "pruning"; and</li>
</ol></li>
<li><ol start="4" type="i">
<li>different weights can be applied the features that comprise the training data.</li>
</ol></li>
</ul>
<p>Disadvantages</p>
<ul>
<li>Over fitting happens when model picks up data with uncommon features value, especially when data is noisy.</li>
</ul>
<h2 id="决策树算法-cart-1">决策树算法-CART</h2>
<p>ID3 和 C4.5是使用基于Entropy-最大信息增益的特征作为节点。</p>
<p>CART代表分类树和回归树，使用基于entropy和ginix index计算信息增益。</p>
<p>Disadvantages</p>
<ul>
<li>It can split on only one variable</li>
<li>Trees formed may be unstable</li>
</ul>
<p>cart的原理就是，构造一颗大树<span class="math inline">\(T_0\)</span>，然后去剪枝（也叫做cost complexity pruning/the weakest link pruning）, 下面以regression tree 和 classification tree举例说明</p>
<blockquote>
<p>如果response var是imbalance, 全部预测为label占比更多的类，怎么办？</p>
</blockquote>
<h3 id="regression-tree">regression tree</h3>
<p>regression tree的cost function 是RSS加上正则项</p>
<p><span class="math display">\[\min\limits_{T\in T_0} \sum\limits_{m=1}^{|T|}\sum\limits_{x_i\in R_m}(y_i - \hat y_{R_m})^2+\alpha|T|\]</span></p>
<ul>
<li><span class="math inline">\(|T|\)</span>是叶子节点的个数。</li>
<li>m表示第m个叶子</li>
<li><span class="math inline">\(R_m\)</span>表示第m个partition region</li>
<li><span class="math inline">\(y_i\)</span>表示第i个样本的真实值</li>
<li><span class="math inline">\(y_{R_m}\)</span>表示第m个partition region的预测值</li>
</ul>
<p>可以使用一个递归的方法来构建一个决策树，主要是要确定partition，或者说分支条件，以及每个落入每个partition（或者说）中对应的预测值。</p>
<figure>
<img src="./img.png" alt="regression tree 构造流程" /><figcaption aria-hidden="true">regression tree 构造流程</figcaption>
</figure>
<h3 id="classification-tree">classification tree</h3>
<p>classification tree切分节点时，参考信息增益，其他流程和构建回归树是一样的</p>
<h1 id="参考">参考</h1>
<ol type="1">
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=s9Um2O7N7YM">决策树算法-linxuantian</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://www.csie.ntu.edu.tw/~htlin/mooc/doc/209_present.pdf">决策树-linxuantian</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6062a083acbfe82c7195b27d/1617076404560/ISLR%2BSeventh%2BPrinting.pdf">An Introduction to Statistical Learning</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://www.quora.com/Why-is-entropy-used-instead-of-the-Gini-index">Why-is-entropy-used-instead-of-the-Gini-index</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/dozercodes/DecisionTree">github-id3的实现1</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/SebastianMantey/Decision-Tree-from-Scratch/blob/master/notebooks/decision_tree_functions.py">github-id3的实现2</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Information_gain_in_decision_trees">wiki-Information_gain_in_decision_trees</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py">sklearn-decisiontree</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://www.quora.com/What-are-the-differences-between-ID3-C4-5-and-CART">quora-ID3-C4-5-and-CART的区别？</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2xudPOBz-vs">youtube-gbdt</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1603.02754.pdf">xgboost</a></p></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" rel="tag"># 人工智能</a>
              <a href="/tags/%E6%A0%91%E6%A8%A1%E5%9E%8B/" rel="tag"># 树模型</a>
              <a href="/tags/%E5%86%B3%E7%AD%96%E6%A0%91/" rel="tag"># 决策树</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/04/15/machine-learning/machine-learning_1-basic/" rel="prev" title="chapter 1 机器学习基础">
                  <i class="fa fa-chevron-left"></i> chapter 1 机器学习基础
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/04/17/machine-learning/machine-learning_2-trees_2/" rel="next" title="chapter 2.2 树模型2-随机森林,Adaboost和GBDT">
                  chapter 2.2 树模型2-随机森林,Adaboost和GBDT <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiechie</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
