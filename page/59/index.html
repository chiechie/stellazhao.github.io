<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"chiechie.github.io","root":"/","images":"/images","scheme":"Muse","version":"8.7.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>
<meta name="description" content="a reader &amp; thinker">
<meta property="og:type" content="website">
<meta property="og:title" content="Chiechie&#39;s Mini World">
<meta property="og:url" content="https://chiechie.github.io/page/59/index.html">
<meta property="og:site_name" content="Chiechie&#39;s Mini World">
<meta property="og:description" content="a reader &amp; thinker">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Chiechie">
<meta property="article:tag" content="博客, AI, 互联网">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://chiechie.github.io/page/59/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/59/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Chiechie's Mini World</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="Chiechie's Mini World" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Chiechie's Mini World</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">Set your course by the stars, not by the lights of passing ships. —— Omar Bradley</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>







</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Chiechie</p>
  <div class="site-description" itemprop="description">a reader & thinker</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">200</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">213</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



          </div>
        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/04/20/machine-learning/machine-learning_4-dl2_practice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/04/20/machine-learning/machine-learning_4-dl2_practice/" class="post-title-link" itemprop="url">chapter 4.2 backpropagation和训练小技巧</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-04-20 10:17:31" itemprop="dateCreated datePublished" datetime="2021-04-20T10:17:31+08:00">2021-04-20</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-11 13:55:24" itemprop="dateModified" datetime="2021-08-11T13:55:24+08:00">2021-08-11</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="反向传播">反向传播</h2>
<blockquote>
<p>** The problem with Backpropagation is that it is a** <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Leaky_abstraction"><strong>leaky abstraction</strong></a><strong>.</strong>---Andrej Karpathy</p>
</blockquote>
<figure>
<img src="https://kratzert.github.io/images/bn_backpass/BNcircuit.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li><p>关于激活函数--sigmoid，另外一个容易忽视的事实是，其梯度在z=0.5时，达到最大--0.25，这意味着，每次梯度信号穿越过sigmoid门时，会衰减为1/4或者更多，如果使用基本 SGD，这会使网络的低层比高层的参数更新慢得多。<img src="https://miro.medium.com/max/1400/1*gkXI7LYwyGPLU5dn6Jb6Bg.png" alt="img" /></p></li>
<li><p>如果网络中使用 sigmoids 或者 tanh 作为激活函数，那么您应该警惕，初始化不会导致想训练过程完全饱和（ fully saturated）。</p></li>
<li><p>非线性函数ReLU：当输入小于0时，导数也为0，此时没有梯度信号能通过激活函数，这个现象叫“dead ReLU” 问题。如果初始权重没选好导致relu的输出为0，那么这个relu神经元再也不会被激活了，就永久保持死亡状态。在训练的过程中发现，大部分神经元（neuron，可能有40%）都是死亡状态。</p></li>
</ol>
<figure>
<img src="https://miro.medium.com/max/1400/1*g0yxlK8kEBw8uA1f82XQdA.png" alt="img" /><figcaption aria-hidden="true">img</figcaption>
</figure>
<ol type="1">
<li>注意：构建的神经网络中有 ReLUs单元时，应该始终对dead ReLUs保持警惕。在训练过程中，如果学习率设置的过于aggressive，常出现relu神经元死亡。</li>
<li>RNNs中的梯度爆炸（Exploding gradients in RNNs）,假设有一个简化的 RNN，不接受输入 x，只递归计算隐藏状态(等价地，输入 x 总是可以为零) ，RNN 是展开了T的时间步长。注意看反向通道时，会看到梯度信号逆着时间传播，并且是通过将隐藏状态乘以同一个矩阵循环矩阵<strong>Whh</strong>传播的，总是被同一个矩阵乘以(递归矩阵 Whh) ，中间穿插着非线性函数的反向传播。</li>
<li>当你用一个数 a 乘以另一个数 b (即 a * b * b * b * b * b * b * b)会发生什么。.)？如果 | b | &lt; 1，这个序列趋近零; 如果 | b | &gt; 1，这个序列趋近无穷。同样的事情也发生在 RNN 的反向传播过程中，不过 b 是一个矩阵，而不仅仅是一个数字，这个时候要算它的最大特征值。</li>
<li><strong>使用RNN时要注意</strong>: 警惕梯度截断（gradient clipping），或者使用LSTM.</li>
<li>额外的发现（Spotted in the Wild: DQN Clipping）：DQN中， 使用 <strong>target_q_t</strong>表示$ [reward * argmax_a Q(s’,a)]$，还有一个变量<strong>q_acted</strong>, which is <strong>Q(s,a)</strong> of the action that was taken. 二者相剑得到一个变量<strong>delta,</strong> 可以用使用l2损失最小化这个变量， <strong>tf.reduce_mean(tf.square()).</strong></li>
<li>损失函数是关于训练数据和网络权重的函数，其中训练数据是常数，权重是变量。因此，虽然损失关于训练数据的梯度很容易算，但是不去算，因为跟目标（更新权重）不一致。算损失关于权重的梯度，从而使用这个梯度去更新权重。</li>
<li>损失函数关于样本的梯度虽然对更新参数没有用，但是可以用来解释模型当前学习到了什么。</li>
<li>导数是什么？随着某个变量的变化，一个函数的变化量。表示函数对于当前变量值的敏感性。</li>
<li>考虑一个多层嵌套的函数f，对其应用链式法则，就可以到达终极变量的导数</li>
</ol>
<ul>
<li><p>f(x,y,z)=(x+y)z可以被分解为:q=x+y 和f=qz</p></li>
<li><p>分开来看，求偏导很简单，∂f/∂q=z, ∂f/∂z=q，∂q/∂x=1，∂q/∂y=1.</p></li>
<li><p>虽然，我们对中间变量的导数不感兴趣，使用链式法则可以沿着中间变量的导数得到f对于终极变量的导数，举个例子：</p>
<p>∂f/∂x=∂f/∂q.∂q/∂x</p>
<p>也就是两个中间变量相关的偏导数的乘积</p></li>
<li><p>激活函数sigmoid的导数比较简洁 <span class="math display">\[ \sigma(x)=\frac{1}{1+e^{-x}}  \rightarrow \frac{d \sigma(x)}{d x}=\frac{e^{-x}}{\left(1+e^{-x}\right)^{2}}=\left(\frac{1+e^{-x}-1}{1+e^{-x}}\right)\left(\frac{1}{1+e^{-x}}\right)=(1-\sigma(x)) \sigma(x) \]</span></p></li>
<li><p><span class="math display">\[f(w, x)=\frac{1}{1+e^{-\left(w_{0} x_{0}+w_{1} x_{1}+w_{2}\right)}}\]</span>,</p></li>
</ul>
<blockquote>
<p>The derivative on each variable tells you the sensitivity of the whole expression on its value.*</p>
</blockquote>
<ul>
<li>导数是函数相对于某个自变量的敏感度；梯度是偏导数组成的向量。</li>
<li>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.</li>
</ul>
<h2 id="layer的输入输出">layer的输入输出</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model_m = Sequential()</span><br><span class="line">model_m.add(Reshape((TIME_PERIODS, num_sensors),</span><br><span class="line">                    input_shape=(input_shape,)))</span><br><span class="line">model_m.add(Conv1D(<span class="number">100</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                   input_shape=(TIME_PERIODS, num_sensors)))</span><br><span class="line">model_m.add(Conv1D(<span class="number">100</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model_m.add(MaxPooling1D(<span class="number">3</span>))</span><br><span class="line">model_m.add(Conv1D(<span class="number">160</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model_m.add(Conv1D(<span class="number">160</span>, <span class="number">10</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model_m.add(GlobalAveragePooling1D())</span><br><span class="line">model_m.add(Dropout(<span class="number">0.5</span>))</span><br><span class="line">model_m.add(Dense(num_classes, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://miro.medium.com/max/2073/1*Y117iNR_CnBtBh8MWVtUDg.png" alt="Image for post" /><figcaption aria-hidden="true">Image for post</figcaption>
</figure>
<ul>
<li>Conv1D：　model_m.add(Conv1D(100, 10, activation='relu', input_shape=(TIME_PERIODS, num_sensors)))：
<ul>
<li>输入：(timesteps, num_series)</li>
<li>输出：(timesteps - kerner_size+1, num_kernels)</li>
<li>参数个数：(kernel_size * num_sensors + 1) * num_kernels</li>
</ul></li>
<li>MaxPooling1D: model_m.add(MaxPooling1D(3))
<ul>
<li>输入：(timesteps, num_series)</li>
<li>输出：(timesteps//3, num_series)</li>
<li>备注：书的页数（channel）没变，但是每页的字数变成了1/3</li>
<li><img src="https://miro.medium.com/max/3058/1*W34PwVsbTm_3EbJozaWWdA.jpeg" title="fig:" alt="Image for post" /></li>
</ul></li>
<li>GlobalAveragePooling1D：model_m.add(GlobalAveragePooling1D())
<ul>
<li>输入：　(timesteps, num_series) 或者（feature_values, feature_detectors）</li>
<li>输出：　（１，num_series）或者（１, feature_detectors）</li>
<li>备注： 书的页数（channel）没变，但是每页的字数变成了1</li>
</ul></li>
<li>Dropout: model_m.add(Dropout(0.5))： 形状不变 输入：（１，num_series） 输出：（１，num_series）</li>
<li>LSTM：model.add(LSTM(units=128, dropout=0.5, return_sequences=True, input_shape=input_shape))
<ul>
<li>输入： (timestep, series)</li>
<li>输出：(timestep, units)</li>
</ul></li>
<li>Dense： model_m.add(Dense(num_classes, activation='softmax'))
<ul>
<li>输入：（１，num_series）</li>
<li>输出：（１，num_classes　×　２）</li>
</ul></li>
<li>BatchNormalization：
<ul>
<li>输入: (timestep, series)</li>
<li>输出：(timestep, series)</li>
<li>参数个数：４ * series (channel)
<ul>
<li><span class="math display">\[y=\gamma\left(\frac{x-\mu(x)}{\sigma(x)}\right)+\beta \]</span></li>
</ul></li>
</ul></li>
</ul>
<h2 id="layer如何设置初始化权重">layer如何设置初始化权重？</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;relu&quot;</span>, kernel_initializer=<span class="string">&quot;he_normal&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure>
<img src="./img.png" alt="3种初始化方法" /><figcaption aria-hidden="true">3种初始化方法</figcaption>
</figure>
<h2 id="训练过程太长">训练过程太长？</h2>
<p>保存训练结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 设置loss，优化算法</span></span><br><span class="line">model.<span class="built_in">compile</span>(loss=..., optimizer=...,</span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line"></span><br><span class="line">EPOCHS = <span class="number">10</span></span><br><span class="line">checkpoint_filepath = <span class="string">&#x27;/tmp/checkpoint&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义回调函数</span></span><br><span class="line">model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(</span><br><span class="line">    filepath=checkpoint_filepath,</span><br><span class="line">    save_weights_only=<span class="literal">True</span>,</span><br><span class="line">    monitor=<span class="string">&#x27;val_accuracy&#x27;</span>,</span><br><span class="line">    mode=<span class="string">&#x27;max&#x27;</span>,</span><br><span class="line">    save_best_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Model weights are saved at the end of every epoch, if it&#x27;s the best seen</span></span><br><span class="line"><span class="comment"># so far.</span></span><br><span class="line"><span class="comment"># 3.在fit中传入回调函数，模型会一边训练一边存储</span></span><br><span class="line">model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 从缓存路径中加载模型</span></span><br><span class="line"><span class="comment"># The model weights (that are considered the best) are loaded into the model.</span></span><br><span class="line">model.load_weights(checkpoint_filepath)</span><br></pre></td></tr></table></figure>
<h2 id="参考">参考</h2>
<ol type="1">
<li><p>Hands-On Machine Learning with Scikit-Learn and TensorFlow, P334</p></li>
<li><p><a target="_blank" rel="noopener" href="https://keras.io/api/callbacks/model_checkpoint/">keras-model_checkpoint-官网文档</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://keras.io/api/layers/merging_layers/concatenate/">concatenate</a></p></li>
<li><p>http://cs231n.github.io/optimization-2/#intuitive</p></li>
<li><p>https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b</p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://chiechie.github.io/2021/04/19/reinforcement_learning/rl1_basic-concepts/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Chiechie">
      <meta itemprop="description" content="a reader & thinker">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chiechie's Mini World">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/04/19/reinforcement_learning/rl1_basic-concepts/" class="post-title-link" itemprop="url">强化学习1 基本概念</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-04-19 20:27:02" itemprop="dateCreated datePublished" datetime="2021-04-19T20:27:02+08:00">2021-04-19</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-08-02 15:47:39" itemprop="dateModified" datetime="2021-08-02T15:47:39+08:00">2021-08-02</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p>相对监督学习，强化学习的神奇之处在于，可能可以超越人类。而监督学习只能模仿，不能超越。</p>
</blockquote>
<h1 id="总结">总结</h1>
<ol type="1">
<li>强化学习是一个序列决策过程，它试图让agent找到一个聪明的策略，使得agent在跟环境互动的过程中，获得最大的长期价值。强化学习中的一些基本概念：
<ol start="2" type="1">
<li>agent和环境：agent就是一个机器人，是我们希望去设计出来的一个AI，环境负责跟AI进行互动，对agent的动作评价，以及生成新的状态。agent接受当前时刻的状态，返回动作agent，环境接受行动，返回下个时刻的的状态，以及奖励。</li>
<li>状态(state)：当前环境，在围棋例子中，表示当前的棋局。</li>
<li>状态转移(state transaction)：状态转移可以是确定的也可以是随机的（随机性来源于环境）</li>
<li>行动(action)表示当前agent的行动空间，在围棋例子中，棋盘上有361个落子位置，对应有361个action</li>
<li>动作空间(action space)是指所有可能动作的集合，在围棋例子中，动作空间是 A = {1, 2, 3, · · · , 361}。</li>
<li>策略函数(policy)：表示agent根据观测到的状态做出决策. 策略函数常常被定义为一个条件概率密度函数，输入当前state，输出动作空间每个动作的概率得分。强化学习的目的，就是希望让agent学习到一个聪明的策略函数。</li>
<li>即时收益（reward)和长期收益（discounted return, aka，cumulative discounted future reward）
<ol type="1">
<li>Reward对于某个&lt;s，a&gt;，是在agent执行一个动作之后，环境返回的一个激励信号，是一个数值记为<span class="math inline">\(R_t\)</span>，取值越大说明越赞赏当前的行动.</li>
<li>Return对于某个&lt;s，a&gt;，即时收益的折扣累积就是长期收益, 记为<span class="math display">\[U_{t}=R_{t}+ \gamma R_{t+1}+\gamma ^{2} R_{t+2}  \cdots\]</span>​</li>
<li>reward和return都是随机变量：reward的随机性来自当前状态和当前行动。return的随机性来自状态转移和 policy。</li>
</ol></li>
</ol></li>
<li>强化学习的方法通常分为两类，基于模型的方法（model-based）和 无模型的方法（model-free）。model-based是强化学习中一种技术，核心思想就是对环境建模，使用计算机构建仿真环境。比如构建一个模拟环境，更新状态和reward。model-based是为了解决样本获取成本高的问题。例如，无人驾驶/无人机，如果让机器在真实交通环境中行驶，并且通过于真实环境互动来获取数据，那必然以发生多次严重车祸作为获取样本的巨大代价。在机器人场景中应用较多。</li>
<li>跟model-based相对的概念是无模型（model-free）方法，它直接通过跟环境交互来获取样本，假设现实中获取样本的成本几乎为0。但是现在的强化学习论文大部分都采用model-free的方法，可见离落地还有一段距离。无模型的方法又大至可分为价值学习和策略学习。</li>
<li>强化学习中，控制agent有两种方法：最大化策略函数和最大化动作价值。</li>
<li>最大化策略函数的方法，也叫策略学习方法，即使用一个神经网络来近似策略函数。更新策略梯度的方法分别有reinforce和actor-critic。</li>
<li>最大化动作价值的方法，也叫价值学习方法，即使用神经网络拟合最优动作价值函数。常用的价值学习方法例如DQN网络，通常采用时序差分的方法去估计网络参数。价值学习高级技巧:对TD算法改进;对神经网络结构改进.</li>
<li>策略梯度中的baseline:策略梯度（Policy gradient） 方法中常用 baseline 来降低方差,加速收敛。Baseline还可应用于REINFORCE 和 A2C 方法中。</li>
<li>连续控制问题即使用随机策略做连续控制。</li>
<li>如何引导agent做出好的策略？定义合适的收益函数，并且对收益求期望--即价值，然后以最大化价值作为目标来训练agent。</li>
<li>强化学习中的价值函数主要有两个作用：一是用来判断agent的策略的好坏，一是用来判断当前局势（state）的好坏。强化学习中有3个价值函数: 行动价值函数，最优行动价值函数，状态价值函数。行动价值函数和最优行动价值函数都是用来判断agengt的动作好坏，状态价值函数用来判断当前局势好坏。</li>
<li>强化学习的应用：神经网络结构搜索；自动生成SQL语句；推荐系统；网约车调度。</li>
</ol>
<h1 id="附录">附录</h1>
<h2 id="强化学习的两个方向-价值学习和策略学习">强化学习的两个方向: 价值学习和策略学习</h2>
<p>价值学习:</p>
<ol type="1">
<li><p>价值学习 (Value-Based Learning) ，是指 以最大化价值函数（3个价值函数都可）为目标去训练agent。</p></li>
<li><p>价值学习的大致思路是这样的，每次agent观测到一个<span class="math inline">\(s_t\)</span>,就把它输入价值函数，让价值函数来对所有动作做评价（分数越高越好）</p></li>
<li><p>那么问题来了，如何去学习价值函数呢？可以使用一个神经网络来近似价值函数。</p></li>
<li><p>最有名的价值学习方法是DQN, 还有Q-learning</p></li>
</ol>
<p>策略学习</p>
<ol type="1">
<li>策略学习指的是学习策略函数<span class="math inline">\(\pi\)</span>, 然后agent可利用策略函数计算不同state下行动的得分，然后随机选一个执行。</li>
<li>那么问题来了，如何去学习策略函数呢？可以使用一个神经网络来近似策略函数，然后使用策略梯度来更新网络参数。</li>
<li>策略梯度算法的代表有REINFORCE</li>
</ol>
<h2 id="价值学习">价值学习</h2>
<p>价值学习： value-based，目的是学习最优行动价值函数。</p>
<ul>
<li>Deep Q network: 近似最优行动价值函数</li>
<li>时间差分（TD）算法:TD（Temporal Difference）算法：
<ul>
<li>SARSA算法：基于表格的方法和基于神经网络的方法</li>
<li>Q-learning算法</li>
<li>Multi-step TD target</li>
</ul></li>
<li>策略学习：policy-based，目的是让agent直接学会最优策略。 actor critic
<ul>
<li>使用策略网络来近似策略函数， 使用策略梯度更新网络参数
<ul>
<li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2Frx6kfw7dc6.png?alt=media&amp;token=22e1d520-3194-42b5-b624-e52034b62b4d" /></li>
</ul></li>
<li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FNC2bv9ZwlF.png?alt=media&amp;token=8fb33ced-8383-42fe-8fee-4742d9abadc4" /></li>
<li><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FV4DSavxJZ8.png?alt=media&amp;token=408e1eb5-24f9-4fd9-bbf5-a7d20a53f7fb" /></li>
<li>计算策略梯度和行动价值函数
<ul>
<li>方法1-跟环境互动获取长期的收益<img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2F8F3Gq0YTEB.png?alt=media&amp;token=d6cd369f-ea76-41b0-84e1-66953c0d4e56" /></li>
<li>方法2-构造价值网络来计算action-value</li>
</ul></li>
</ul></li>
<li>价值学习和策略学习结合： actor critic <img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FQX2HjRc5gn.png?alt=media&amp;token=18a877c8-f337-4c15-a9f5-7b3f410c8475" /></li>
</ul>
<h2 id="三个价值函数">三个价值函数？</h2>
<p>强化学习中的价值函数主要有两个作用：一是用来判断agent的策略的好坏，一是用来判断当前局势（state）的好坏。强化学习中有3个价值函数: 行动价值函数，最优行动价值函数，状态价值函数</p>
<ol type="1">
<li><p>动作价值函数（action-value function）表示在当前状态s下，采取行动a的收益如何, 在采取某个策略<span class="math inline">\(\pi\)</span>时<span class="math inline">\(Q_{\pi}\left(s_{t}, a_{t}\right)=E_{\pi}\left(U_{t} \mid S_{t}=s_{t}, A_{t}=a_{t}\right)\)</span></p></li>
<li><p>最优动作价值函数(optimal action-value function)表示采取最优策略<span class="math inline">\(\pi^{\star}\)</span>时，给当前的&lt;s, a&gt;打分</p>
<p><span class="math inline">\(Q^{*}\left(s_{t}, a_t\right)=\max\limits_{\pi} Q_{\pi}\left(s_{t}, a_t\right)\)</span></p></li>
<li><p>状态价值函数(state value function):在后续采取某个策略<span class="math inline">\(\pi\)</span>的情况下，当前局势的好坏（是快赢了还是快输了）。用于给当前state打分。如果对s求期望<span class="math inline">\(\mathbb{E}_{S}\left[V_{\pi}(S)\right]\)</span>，就是对策略<span class="math inline">\(\pi\)</span>打分。<span class="math inline">\(V_{\pi}\left(s_{t}\right)=\operatorname{E_A}\left(Q_{\pi}\left(s_{t}, A\right)\right), A \sim \pi(.\mid s_t)\)</span></p>
<ul>
<li>如果动作（action）是离散变量：<span class="math inline">\(V_{\pi}\left(s_{t}\right) = \sum\limits_{a}Q_{\pi}(s_t,a) \pi\left(a \mid s_{t}\right)\)</span></li>
<li>如果动作（action）是连续变量： <span class="math inline">\(V_{\pi}\left(s_{t}\right) = \int_{a} Q_{\pi} \left(s_{t}, a\right) \pi\left(a \mid s_{t} \right) da\)</span></li>
</ul></li>
<li><p>总结一下，动作价值函数和最优动作价值函数是来给&lt;state,action&gt;打分，状态价值函数是在给当前&lt;state，&gt;打分，不涉评价动作。</p></li>
</ol>
<p><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FDl41z9c-9y.png?alt=media&amp;token=d5e65193-4372-4d43-b8c4-85237c20b61d" /></p>
<h2 id="控制agent的两种方法-基于策略和基于最优动作价值函数">控制agent的两种方法-基于策略和基于最优动作价值函数</h2>
<p><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FtnK44wspcQ.png?alt=media&amp;token=259a4682-aa14-4b7d-8f55-e88d29cdb319" /></p>
<p><img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Frf_learning%2FJf9FJZ0nSH.png?alt=media&amp;token=8fb09202-0693-4658-9c45-d2bec3f8642c" /></p>
<h2 id="qa">Q&amp;A</h2>
<h3 id="价值学习策略学习动作裁判学习-这三种方法-存在的必要">价值学习，策略学习，动作裁判学习 这三种方法 存在的必要？</h3>
<ul>
<li>既然有了policy Function 或者 Q, 都可以告诉agent怎么操作。那有什么必要去做 actor-critic 算法呢？单独学两个神经网络不就可以了？</li>
<li>问题就是不容易学呀！所以才会有Actor-Critic这种复杂的方法。</li>
</ul>
<h3 id="强化学习-适合哪些应用场景图像分类-适合用吗为什么-auto-ml适合用">强化学习 适合哪些应用场景？图像分类 适合用吗？为什么 auto-ml适合用？</h3>
<ul>
<li>适合 序列决策 问题， 图像分类没必要 用</li>
<li>训练一个大的神经网络类似玩一局游戏 耗时很久，每一个迭代 类似 游戏里面的 一小步，可以拿到即时reward，这样就可以使用TD算法来学习 value-function。</li>
</ul>
<h3 id="在线学习-和-强化学习的-区别">在线学习 和 强化学习的 区别？</h3>
<ul>
<li>online learning 不假定模型输出会影响未来输入，只看单步损失</li>
<li>reinforcement learning 中模型输出会影响未来输入，必须考虑输出的后果</li>
</ul>
<h3 id="on-policy-和-off-policy-的-区别">on-policy 和 off-policy 的 区别？</h3>
<ul>
<li>相同点：都是用来更新价值网络的学习算法</li>
<li>不同点是：on-policy是按照当前的policy来估计q-value；off-policy是按照最优的policy来估计q-value</li>
</ul>
<h3 id="监督学习-和-强化学习的区别">监督学习 和 强化学习的区别</h3>
<ol type="1">
<li>监督学习要求数据 是独立同分布的，学习过程必须要有老师手把手教，标准答案（label）是什么，</li>
<li>强化学习数据不用iid，没有导师（supervisor），不会被告知正确的action是什么，只有一个奖励信号，并且有延迟的。需要自己去试错，找到具长期reward最大的action</li>
<li>强化学习的本质优势在于，不要求反馈信号相对策略参数可导，甚至连反馈信号跟策略的表达式都不需要有，而这也是真实应用中的情况，通常的解决方案是构建模拟环境，以期低成本地搜集海量样本。</li>
</ol>
<h3 id="强化学习的特点">强化学习的特点</h3>
<ul>
<li><p>强化学习中随机性的两个来源：action可以是随机的，状态转移可以是随机的</p></li>
<li><p>试错机制</p></li>
<li><p>延迟reward</p></li>
<li><p>时间会产生影响</p></li>
<li><p>agent的行为会影响后续收到的数据（agent的action改变了环境）</p>
<blockquote>
<p>类似我们的模型自动更新，担心时间会对模型造成影响。比如模型推荐了商品a，我们也只能收到关于商品a的反馈。</p>
</blockquote></li>
</ul>
<h2 id="参考资料">参考资料</h2>
<ol type="1">
<li><a target="_blank" rel="noopener" href="https://github.com/wangshusen/DRL/blob/master/Slides/1_Basics_1.pdf">wangshusen-slide</a></li>
<li><a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/184657/what-is-the-difference-between-off-policy-and-on-policy-learning">on-policy和off-policy的区别-stackoverflow</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/290530992">深度强化学习 在实际应用中少吗？难点在那里？</a></li>
<li><a target="_blank" rel="noopener" href="https://weibo.com/titaniumviii?refer_flag=0000015010_&amp;from=feed&amp;loc=nickname">宋一松SYS</a></li>
<li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/64526936">强化学习和在线学习的区别-zhihu</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/zhoubolei/introRL">zhoubolei-github-课程资料</a> ｜<a target="_blank" rel="noopener" href="https://github.com/zhoubolei/introRL/blob/master/lecture1.pdf">slide</a></li>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1LE411G7Xj">zhoubolei视频</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangshusen/DRL/blob/master/Notes_CN/DRL.pdf">wangshusen-强化学习中文教材</a></li>
<li><a target="_blank" rel="noopener" href="https://youtu.be/vmkRMvhCW5c">wangshusen-视频</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangshusen/DeepLearning">深度学习课件</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangshusen/DRL">深度强化学习-notes</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangshub/RL-Stock">使用强化学习炒股</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/267998242">强化学习应用于金融问题的文章</a></li>
<li><a target="_blank" rel="noopener" href="https://gym.openai.com/">Gym-强化学习开发框架</a>：开发强化学习算法的工具箱，有很多第三方环境</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/58/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><span class="page-number current">59</span><a class="page-number" href="/page/60/">60</a><span class="space">&hellip;</span><a class="page-number" href="/page/100/">100</a><a class="extend next" rel="next" href="/page/60/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Chiechie</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
